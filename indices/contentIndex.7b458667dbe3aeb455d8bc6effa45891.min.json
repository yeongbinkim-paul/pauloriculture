{"/":{"title":"ü™¥ Pauloriculture","content":"\n## Nice to meet you.\n## I'm Yeongbin (Paul). I'm a data engineer.\n\u003cimg src=\"pfp.png\" width=\"350\"\u003e\n\n## Welcome to My Brain Gardenü™¥\n\n*‚ÄúThe data product will set me free‚Äù*\n\nI am a Driven Data Engineer with 2+ years of experience in crafting data pipelines and backend systems.\n\nProven ability to write clear, well-documented code, enhancing maintainability and troubleshooting efficiency.\n\nExhibited proficiency in independent problem-solving using data-driven approaches.\n\nLeveraged critical thinking and communication skills for efficient cross-functional, interpersonal collaboration.\n\nAlways Ambitious to grow professionally alongside esteemed colleagues and the company with a given mission.\n\nLanguages : English | Korean\n\n## Navigation...\nThere are multiple ways to navigate my Brain Garden:\n\n1. Use the search bar on the top right or press `cmd+k` (`ctrl+k` on Windows) or click on the Search button (top right) to search for any term.\n2. Click on a note to explore its content, and follow the links and backlinks to dive deeper into related topics.\n3. Interact with the graph at the bottom of the page to visualize connections between notes and click on any node to navigate directly to that note.\n4. Click on the [Hashtags](tags) to explore the topics by tags.\n\n## Where can you find me?\n\n**Current Location** : Toronto, Ontario\n\n**My Personal Email** : kimyb132@gmail.com\n\n**Links** : [LinkedIn](https://www.linkedin.com/in/yeongbin-kim-972635118/) | [Github](https://github.com/yeongbinkim-paul)\n\n\n\n| Main Tags          |\n|---------------|\n| [Ideas](tags/idea),[Data Engineering](tags/data-engineering), [SQL](tags/sql), [Pandas](tags/pandas), [EN](tags/EN)  |\n\n\u003c!--\n### Data Engineering Idea Notes (EN)\n1. [Idea for good Data Warehousing (EN)](notes/data_engineering/idea_for_good_data_warehousing_en.md)\n\n### Data Engineering Study Notes (EN)\n1. [Pandas as and SQL (EN)](notes/data_engineering/pandas_as_an_sql_en.md)\n\n### Data Analysis Study Notes (EN)\n1. [Dune Analytics : NFT Market Overview (EN)](notes/dune_nft/market_overview_en.md)\n\n### Data Engineering Idea Notes (KR)\n1. [Idea for good Data Warehousing (KR)](notes/data_engineering/idea_for_good_data_warehousing_kr.md)\n\n### Data Engineering Study Notes (KR)\n1. [Pandas as and SQL (KR)](notes/data_engineering/pandas_as_an_sql_kr.md)\n\n### Data Analysis Study Notes (KR)\n1. [Dune Analytics : NFT Market Overview (KR)](notes/dune_nft/market_overview_kr.md) --\u003e\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/idea_for_good_data_warehousing_en":{"title":"[DE] Idea for good Data Warehousing (EN)","content":"# Go to Korean Edition [Idea for good Data Warehousing (KR)](notes/data_engineering/idea_for_good_data_warehousing_kr.md)\n\n# Idea for good Data Warehousing (EN)\nA good data engineer should be able to manage a data warehouse well.\nBecause although many say that the scope of what a data engineer does is too broad, the most important task is building a good place to store and work with data.\nToday I've written a few important things to remember while building and managing a data warehouse.\n\n## What is Data Warehouse?\nLet's start with the definition of a data warehouse.\n\nFirst introduced in 1992 in the book Building the Data Warehouse by Inmon W.H., a data warehouse is where data is gathered from multiple data sources, subject-oriented, and integrated. Inmon also says that a data warehouse should be time-variant, meaning that all data can be identified in a specific time interval, and non-volatile, meaning that data can be added but not erased to maintain a consistent purpose.\n\nThis data warehouse is defined in its most general scope, so there is a difference between that and the data warehouses that do the job nowadays.\nThis is because today's data warehouses are complete with many other external factors, including the purpose of the data and the cost of storing and processing it.\n\nI will talk about some things that can be added to this definition that are important to consider when designing and building a data warehouse.\n\n## Domain knowledge for Designing a Data Warehouse\nDomain knowledge is critical because data engineers should work with data based on well-developed logic.\nIf data engineers have a background in where the data is being used, they can distinguish the correlation and causation of each piece of data. If not, this will not be treating the data based on logic. It will be data processing to make it look good and will not serve any purpose.\nThat's why as a data engineer, you must have good domain knowledge and try to understand all of the data you're dealing with, in what form it exists, and how it's used.\n\nWhile this may seem like a similar perspective to domain-driven design, it's a little different.\nIt's well known that you can achieve high productivity, low complexity, and easy communication by adopting a domain-driven design in general. A data warehouse with domain knowledge can achieve various performance optimizations that best reflect the current data characteristics while keeping the data as usable as possible.\n\n\n## Demand forecasting in the data warehouse design process\nIf there is one remaining challenge for data engineers with good domain knowledge, it's demand forecasting.\nDemand forecasting refers to forecasting the demand for data that consumers will frequently use in the data warehouse for analytical purposes and preparing that data in a usable form.\n\nIn creating a data product, you must draw insights from multiple data points. As a data engineer, you work closest to the data sources, so you have the first access to this data and can easily do the most basic analysis.\nMost data is multidimensional, meaning that the same data can be separated into multiple dimensions and used for analysis. When dealing with multidimensional data, you must analyze multiple dimensions to see correlations.\nHowever, not all correlations lead to meaningful causal analysis, so data engineers must have many conversations with consumers who use the data they create.\n\n*Often, the consumers, in this case, will be people in cross-functional roles, such as data analysts, backend engineers, project managers, and product managers. These people have the same goal: to work with data, so they should have a broad understanding of a well-defined goal.*\n\nThen you should get ideas of how your data pipeline stores data and how it should be updated.\n\n## If I create a data warehouse architecture,\nAs described above, we believe that a data engineer should have a well-rounded understanding of the data first and foremost, based on well-developed domain knowledge, so that they can understand the consumer's needs and then design the data warehouse architecture accordingly.\nFollow this process carefully to save time refactoring your data warehouse architecture to respond to different data demands and changing your data schema occasionally.\n\nSo, if I were designing a data warehouse architecture from scratch, here's what I would consider.\n\n### 1. Understand as much as you can about the data you have right now.\nData architectures designed without fully understanding the data on your desk are short-lived. As I said above, you need to take the time to understand the data you have and what you need to accomplish by communicating with the consumers of that data.\n\n### 2. Determine what you must accomplish first and how you will get there.\nEvery data architecture has a business feasibility and a concise, maintainable architecture that considers all possibilities doesn't exist in the real world. First and foremost, you need to consider how you can accomplish what you need to complete, and in doing so, adopt the easiest and least expensive tech spec and implementation possible because the business logic will inevitably become more complex.\n\nHere are some of the things I like to consider during this process\n\n#### Requirements from the consumer perspective:\n\nPlease communicate with the data analysts and backend engineers who will consume the output of your data pipeline to understand their requirements. There should be an agreement on the information that must be included, the data format, and the frequency of data updates. This could take the form of an SLA. Ideally, if you're looking for a flawless implementation, you should be able to break down achievable goals with business feasibility, calculate a timeline, and get the work done.\n\n#### Selecting tech specs:\n\nYou need to select a tech spec for the implementation, which includes:\n\n1. the data infrastructure architecture\n\n2. the technical familiarity of the team members with the data infrastructure\n\n3. a weekly/monthly cost estimate for the data infrastructure\n\n4. a time estimate for the implementation.\n\nThe problem is that all the factors are highly uncertain, and it's hard to determine the order of importance. This is because the problem you're trying to solve looks different for every team in every company.\n\nI think the first thing to do in this situation is to do a Pre-Mortem with your team.\nIn his book 'Thinking, Fast and Slow', Daniel Kahneman refers to the concept of the planning fallacy, stating that many people become overconfident and fail to carry out their plans by creating unrealistic best-case scenarios or ignoring outside perspectives to find examples of similar situations.\nTo address the optimism that causes it, he suggests Pre-Mortem. Applying this concept to our problem, we can proceed as follows.\n\n1. Get together with your team and explain why this tech spec will fail miserably in 1, 3, or 6 months.\n\n2. For each reason for failure, identify ways to prevent it in the planning process.\n\n3. Briefly explore external examples that address each reason for failure.\n\nThese steps can prevent the team from collectively conforming to a decision and allow knowledgeable people to use their imaginations to identify risks in a desirable direction.\nIf you go through this process and then go back and select a tech spec that best satisfies the four considerations above, you will have a much more reliable tech spec.\n\n### 3. Plan your data stages.\nFor each stage, you need to define the purpose of the executed ETL jobs and the resulting data's role.\n\nFor example, one way to plan is to divide the stages into DATA LAKE, where data sources are loaded; DATA WAREHOUSE, which contains processed data; and DATA MART, which the consumers will use for actual products or analytics. The clearer the role of the pipe that processes data in each stage, the better it will be to identify improvement points when improving the structure in the future.\n\nHere's the guiding principle\n\n1. Rules for Storing data:\n\n    Each stage should have clear rules for storing data. For example, not all stages need to be non-volatile, but Source must be non-volatile.\n\n2. Rules for categorizing data:\n\n    Make sure you have clear rules for categorizing data in each stage. For example, the nature of data in a DATA MART can be data with new information generated from multiple data sources or data that has a clearly defined use outside of the pipeline. Be clear about the rationale for categorizing and storing data in each stage.\n\n3. Rules for processing data:\n\n    Make sure you have clear rules for processing data in each stage. For example, a pipeline that processes data in the stage before the DATA MART can enforce logic that guarantees equality. Or, when processing data in the data lake, you can enforce rules that only allow changing data type or column name.\n\n4. Rules for handling exception:\n\n    Respond to any exceptions you may find to the above rules based on business feasibility, but make sure that all exceptions are recorded as re-architecting points so that they can be revisited.\n\nThe trickiest part of these rules is the data categorization rules. For example, the tables in DATA MART are used for Products as they are, but you can join multiple mart tables to create a new table for analysis.\n\nYou can make the following judgments to deal with these cases.\n\n1. Consider whether other sources can be used to produce data for analytical purposes instead of using DATA MART tables as sources.\n\n2. You should be able to consider whether you can build a new DATA WAREHOUSE with DATA MART tables as its source for analytical purposes.\n\nDepending on your business situation, I'm sure there are other options to consider, but this is where data engineers grow from much experience and study.\n\n### 4. Plan how you will manage your data lineage.\nThe more complex your business logic becomes, the more likely you are to have a single point of failure due to the people who planned the data architecture in the first place, so you need to think about how to manage Data Lineage in a way that anyone can learn. If it's difficult for everyone to learn, you will end up with SPF.\n\nTo ensure that to configure Data Lineage in a way that doesn't hurt your productivity, we recommend avoiding the following situations as much as possible.\n\n1. If you must create a cyclic structure within one data warehouse.\n\n2. When processing data by referring to sources contained in different data stages\n\nBoth situations would undermine the definition of the data stages we planned in #3.\nThese problems are best solved by leveraging domain knowledge, business goals, and stakeholder communication because complex data lineage will eventually become a bottleneck for data productivity.\n\nThere are choices here that you should avoid at all costs.\n\n1. The solution of changing how data analysts or data backend engineers handle things:\n\n    This solution will eventually lead to you facing the same problem again.\n\n2. Choosing complexity because it is unavoidable and utilizing the resulting data as a source for subsequent data:\n\n    If you choose to allow exceptions because complexity is an inevitable feature of your structure, you must control that the exceptions allowed do not increase complexity downstream.\n\n### 5. Plan how you will validate, verify, and monitor your data.\nWe will always encounter unexpected exception data, even with good data architecture.\nThese exceptions can happen by complex logic that exceeds the scope of allocated resources, outlier inputs that don't have a business impact, or schema changes or version upgrades to the data source.\n\nData quality is more important to data engineers than anything else, so they must be the first to identify and respond to possible exceptions for all data-handling pipelines.\nSo, to complete a good data architecture, you need to decide how to organize your validation and verification and how to monitor it.\n\nHere's what I consider important about Validation, Verification, and Monitoring.\n\n1. You should be able to verify that it follows the definition of a Stage.\n\n2. Depending on the data lineage, you should ensure each validation logic that verifies data for a specific point. You should not perform validation of the same objective at multiple stages.\n\n3.  You should separately record exception data identified during the validation process and leverage it in the verification rule.\n\n4. You should monitor by identifying the points where the most exceptions occurred during development but should evolve through issues during production to avoid gray areas.\n\n## Conclusion\nThere isn't a right answer to good data architecture because better technologies are always introduced into the world, and more creative solutions can be found. While creating that answer, we should always be careful about what we must catch. What we achieve may not be the best, it may be the second best, but we can create another best if we aim for best practices and don't forget what we need to consider to do that.\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/idea_for_good_data_warehousing_kr":{"title":"[DE] Idea for good Data Warehousing (KR)","content":"# Go to English Edition [Idea for good Data Warehousing (EN)](notes/data_engineering/idea_for_good_data_warehousing_en.md)\n\n# Idea for good Data Warehousing (KR)\nÏ†ÄÎäî Ï¢ãÏùÄ Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Îäî Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Î•º Ïûò Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏñ¥Ïïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÏôúÎÉêÌïòÎ©¥, ÎπÑÎ°ù ÎßéÏùÄ ÏÇ¨ÎûåÎì§Ïù¥ Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Í∞Ä Ìï¥Ïïº ÌïòÎäî ÏòÅÏó≠Ïù¥ ÎÑàÎ¨¥ ÎÑìÎã§Í≥† ÎßêÌïòÏßÄÎßå, Í∑∏ Ï§ëÏóêÏÑúÎèÑ Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥† Îã§Î£®Îäî Í≥µÍ∞ÑÏùÑ Ïûò Íµ¨Ï∂ïÌïòÎäî Í≤ÉÏù¥ Í∞ÄÏû• Ï§ëÏöîÌïú ÏûÑÎ¨¥ÎùºÍ≥† ÏÉùÍ∞ÅÌïòÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\nÏò§ÎäòÏùÄ Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Î•º Íµ¨Ï∂ïÌïòÍ≥† Í¥ÄÎ¶¨ÌïòÎ©¥ÏÑú Íº≠ Ïã†Í≤ΩÏç®Ïïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌïòÎäî Í≤ÉÎì§Ïóê ÎåÄÌï¥ÏÑú Î™á Í∞ÄÏßÄ Ï†ÅÏñ¥Î≥¥ÏïòÏäµÎãàÎã§.\n\n## Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Ïùò Ï†ïÏùò\nÍ∞ÄÏû• Î®ºÏ†Ä Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Ïùò Ï†ïÏùòÎ∂ÄÌÑ∞ ÏÇ¥Ìé¥Î¥ÖÎãàÎã§.\n\n1992ÎÖÑ Inmon W.HÏùò Ï±Ö (Building the Data Warehouse)ÏóêÏÑú Ï≤òÏùå ÏÜåÍ∞úÎêú Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ÎûÄ ÌöåÏÇ¨Ïùò Ïö¥ÏòÅ Î∞©Ìñ• ÎåÄÏã† ÌäπÏ†ï Ï£ºÏ†úÏóê ÎßûÎäî Îç∞Ïù¥ÌÑ∞Î•º ÏßÄÎãàÍ≥† ÏûàÏúºÎ©∞ (Subject Oriented), Ïó¨Îü¨ Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§Î°úÎ∂ÄÌÑ∞ ÏàòÏßëÎêú Îç∞Ïù¥ÌÑ∞Îì§Ïù¥ Î™®Ïù¥Îäî Í≥≥ÏûÖÎãàÎã§ (Integrated). InmonÏùÄ ÎòêÌïú, Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Îäî Î™®Îì† Îç∞Ïù¥ÌÑ∞Í∞Ä Í∞ÅÍ∞Å ÌäπÏ†ï ÏãúÍ∞Ñ Íµ¨Í∞ÑÏóêÏÑú ÏãùÎ≥ÑÎê† Ïàò ÏûàÏñ¥Ïïº ÌïòÎ©∞ (Time-variant), Îç∞Ïù¥ÌÑ∞Í∞Ä Ï∂îÍ∞ÄÎê† Ïàò ÏûàÏúºÎ©¥ÏÑú ÏßÄÏõåÏßÄÏßÄ ÏïäÏïÑ ÏùºÍ¥ÄÎêú Î™©Ï†ÅÏùÑ Ïú†ÏßÄÌï† Ïàò ÏûàÎäî ÎπÑÌúòÎ∞úÏÑ± (Non-volatile)ÏùÑ Í∞ñÏ∂∞Ïïº ÌïúÎã§Í≥† ÎßêÌï©ÎãàÎã§.\n\nÏ†ÄÎäî Ïù¥Í≤ÉÏù¥ Í∞ÄÏû• ÏùºÎ∞òÏ†ÅÏù∏ Î≤îÏúÑÏóêÏÑú Ï†ïÏùòÎêú Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê ÌòÑ ÏãúÎåÄÏóê Ïó≠Ìï†ÏùÑ ÏàòÌñâÌïòÎäî Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Îì§Í≥ºÎäî Î∂ÑÎ™ÖÌûà Ï∞®Ïù¥Í∞Ä Ï°¥Ïû¨ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÌòÑÏû¨Ïùò Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ÏóêÎäî Îç∞Ïù¥ÌÑ∞Ïùò Î™©Ï†ÅÍ≥º, Ï†ÄÏû• Î∞è Ï≤òÎ¶¨ ÎπÑÏö©ÏùÑ Ìè¨Ìï®ÌïòÎäî Îã§Î•∏ Îã§ÏñëÌïú Ïô∏Î∂Ä ÏöîÏù∏Îì§ÍπåÏßÄ Ìè¨Ìï®ÌïòÏó¨ ÏôÑÏÑ±ÎêòÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n\nÏ†ÄÎäî Ïù¥ Ï†ïÏùòÏóê ÎçßÎ∂ôÏùº Ïàò ÏûàÎäî, Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Î•º ÏÑ§Í≥ÑÌï† Îïå Ï§ëÏöîÌïòÍ≤å Í≥†Î†§Ìï¥Ïïº ÌïòÎäî Í≤ÉÎì§ÏùÑ Î™á Í∞ÄÏßÄ Ïù¥ÏïºÍ∏∞ÌïòÎ†§Í≥† Ìï©ÎãàÎã§.\n\n## Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏÑ§Í≥ÑÎ•º ÏúÑÌïú ÎèÑÎ©îÏù∏ ÏßÄÏãù\nÏ†ÄÎäî Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Í∞Ä Ïûò Í∞ñÏ∂∞ÏßÑ ÎÖºÎ¶¨Ïóê Í∑ºÍ±∞Ìï¥ Îç∞Ïù¥ÌÑ∞Î•º Îã§Î§ÑÏïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌïòÍ∏∞ ÎïåÎ¨∏Ïóê ÎèÑÎ©îÏù∏ ÏßÄÏãùÏùÄ ÍµâÏû•Ìûà Ï§ëÏöîÌïòÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÎç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Í∞Ä Îç∞Ïù¥ÌÑ∞Í∞Ä Ïì∞Ïù¥Îäî Í≥≥Ïóê ÎåÄÌïú Î∞∞Í≤ΩÏßÄÏãùÏù¥ ÏóÜÎã§Î©¥, Í∞ÅÍ∞ÅÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä Í∞ÄÏßÑ ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÏôÄ Ïù∏Í≥ºÍ¥ÄÍ≥ÑÎ•º Ï†úÎåÄÎ°ú Íµ¨Î∂ÑÌïòÏßÄ Î™ªÌïòÍ≤å Îê©ÎãàÎã§. Ïù¥Îäî ÎÖºÎ¶¨Ïóê Í∑ºÍ±∞Ìï¥ Îç∞Ïù¥ÌÑ∞Î•º Îã§Î£®Îäî Í≤ÉÏù¥ ÏïÑÎãàÎùº Î≥¥Í∏∞ Ï¢ãÍ≤å Íæ∏Î©∞ÏßÑ Îç∞Ïù¥ÌÑ∞ Í∞ÄÍ≥µÏù¥ ÎêòÏñ¥ Ïñ¥Îñ§ Ìö®Ïö©ÎèÑ ÏóÜÍ≤å ÎßåÎì§ Í≤ÉÏûÖÎãàÎã§.\nÏ†ÄÎäî Í∑∏Î†áÍ∏∞ ÎïåÎ¨∏Ïóê Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥ÎùºÎ©¥ ÎèÑÎ©îÏù∏ ÏßÄÏãùÏùÑ Ïûò Í∞ñÏ∂îÍ≥†, Îã§Î§ÑÏïº Ìï† Îç∞Ïù¥ÌÑ∞Îì§Ïù¥ Ïñ¥Îñ§ ÌòïÌÉúÎ°ú Ï°¥Ïû¨ÌïòÎäî ÏßÄ, Í∑∏Î¶¨Í≥† Ïñ¥Îñ§ Î∞©Ìñ•ÏúºÎ°ú Ïì∞Ïù¥Îäî ÏßÄÍπåÏßÄ Ï†ÑÎ∂Ä Ïù¥Ìï¥ÌïòÎ†§Í≥† ÎÖ∏Î†•Ìï¥Ïïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\nÎèÑÎ©îÏù∏ Ï£ºÎèÑ ÏÑ§Í≥ÑÏôÄ ÎπÑÏä∑Ìïú Í¥ÄÏ†êÏúºÎ°ú Î≥¥Ïùº Ïàò ÏûàÏßÄÎßå, Ï°∞Í∏àÏùÄ Îã§Î•∏ Ï†êÏù¥ ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÎèÑÎ©îÏù∏ Ï£ºÎèÑ ÏÑ§Í≥ÑÎ•º Ï±ÑÌÉùÌïòÎ©¥ ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÎÜíÏùÄ ÏÉùÏÇ∞ÏÑ±Í≥º ÎÇÆÏùÄ Î≥µÏû°ÎèÑ, Ïâ¨Ïö¥ Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖòÏùÑ Îã¨ÏÑ±Ìï† Ïàò ÏûàÎã§Í≥† Ïûò ÏïåÎ†§Ï†∏ ÏûàÏäµÎãàÎã§. ÎèÑÎ©îÏù∏ ÏßÄÏãùÏùÑ Í∞ñÏ∂ò Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§Îäî Îç∞Ïù¥ÌÑ∞Ïùò ÌôúÏö© Î≤îÏúÑÎ•º Í∞ÄÎä•Ìïú Ìïú ÏµúÎåÄÎ°ú Ïú†ÏßÄÌï† Ïàò ÏûàÏúºÎ©¥ÏÑú, ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞ ÌäπÏÑ±ÏùÑ Í∞ÄÏû• Ïûò Î∞òÏòÅÌïòÎ©¥ÏÑú Îã§ÏñëÌïú ÏÑ±Îä• ÏµúÏ†ÅÌôîÎ•º Îã¨ÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n\n## Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏÑ§Í≥Ñ Í≥ºÏ†ïÏóêÏÑúÏùò ÏàòÏöî ÏòàÏ∏°\nÎèÑÎ©îÏù∏ ÏßÄÏãùÏùÑ Ïûò Ïù¥Ìï¥Ìïú Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥ÏóêÍ≤å Îòê ÌïòÎÇò ÎÇ®ÏùÄ Í≥ºÏ†úÍ∞Ä ÏûàÎã§Î©¥, ÏàòÏöî ÏòàÏ∏°Ïù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÏàòÏöî ÏòàÏ∏°ÏùÄ Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ÏóêÏÑú ÏÜåÎπÑÏûêÎì§Ïù¥ Î∂ÑÏÑù Î™©Ï†ÅÏúºÎ°ú ÏûêÏ£º ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞Ïùò ÏàòÏöîÎ•º ÏòàÏ∏°ÌïòÏó¨ Í∑∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÍ∏∞ Ï¢ãÏùÄ ÌòïÌÉúÎ°ú Ï§ÄÎπÑÌïòÎäî Í≥ºÏ†ïÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.\n\nÎç∞Ïù¥ÌÑ∞ ÌîÑÎ°úÎçïÌä∏Î•º ÎßåÎìúÎäî Í≥ºÏ†ïÏóêÏÑúÎäî Ïó¨Îü¨ Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏Î°úÎ∂ÄÌÑ∞ Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º ÎèÑÏ∂úÌï¥Ïïº Ìï©ÎãàÎã§. Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Îäî Í∞ÄÏû• Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ÏôÄ Î∞ÄÏ†ëÌïú ÏúÑÏπòÏóêÏÑú ÏùºÌïòÎäî ÏÇ¨ÎûåÏù¥Í∏∞ ÎïåÎ¨∏Ïóê, Ïù¥ Îç∞Ïù¥ÌÑ∞Îì§Ïóê Í∞ÄÏû• Î®ºÏ†Ä Ï†ëÍ∑ºÌï† Ïàò ÏûàÍ≥†, Í∞ÄÏû• Í∏∞Î≥∏Ï†ÅÏù∏ Î∂ÑÏÑùÏùÑ ÏâΩÍ≤å Ìï¥ÎÇº Ïàò ÏûàÎäî Ïó≠Ìï†Ïóê ÏúÑÏπòÌï¥ ÏûàÏäµÎãàÎã§.\nÎåÄÎ∂ÄÎ∂ÑÏùò Îç∞Ïù¥ÌÑ∞Îäî Îã§Ï∞®Ïõê Îç∞Ïù¥ÌÑ∞ ÌòïÌÉúÎ•º Í∞ñÍ≥† ÏûàÏäµÎãàÎã§. Í∞ôÏùÄ Îç∞Ïù¥ÌÑ∞ÎùºÎèÑ Ïó¨Îü¨ Ï∞®ÏõêÏúºÎ°ú Î∂ÑÎ¶¨ÎêòÏñ¥ Î∂ÑÏÑùÏóê ÏÇ¨Ïö©Îê† Ïàò ÏûàÎã§Îäî ÏùòÎØ∏ÏûÖÎãàÎã§.\nÎã§Ï∞®Ïõê Îç∞Ïù¥ÌÑ∞Î•º Îã§Î£¨Îã§Î©¥, Í∑∏ ÏÜçÏóêÏÑú Ïó¨Îü¨ Ï∞®ÏõêÏóê ÎåÄÌïú Î∂ÑÏÑùÏùÑ ÌÜµÌï¥ Îã§ÏñëÌïú ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º ÌååÏïÖÌï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\nÍ∑∏Î†áÏßÄÎßå Î™®Îì† ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä Ïú†ÏùòÎØ∏Ìïú Ïù∏Í≥ºÍ¥ÄÍ≥Ñ Î∂ÑÏÑùÏúºÎ°ú Ïù¥Ïñ¥ÏßÄÏßÄ ÏïäÏäµÎãàÎã§. Í∑∏Î†áÍ∏∞ ÎïåÎ¨∏Ïóê Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Îäî ÏûêÏã†Ïù¥ ÎßåÎì† Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÎäî ÏÜåÎπÑÏûêÏôÄ ÎßéÏùÄ ÎåÄÌôîÎ•º ÎÇòÎàå Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n*ÎåÄÍ∞ú Ïù¥ Í≤ΩÏö∞Ïóê ÏÜåÎπÑÏûêÎäî Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∞Ä, Î∞±ÏóîÎìú ÏóîÏßÄÎãàÏñ¥, ÌîÑÎ°úÏ†ùÌä∏ Îß§ÎãàÏ†Ä, ÌîÑÎ°úÎçïÌä∏ Îß§ÎãàÏ†ÄÏ≤òÎüº Îã§ÏñëÌïú Í∏∞Îä•Ïùò Ïó≠Ìï†ÏùÑ ÏàòÌñâÌïòÎäî ÏÇ¨ÎûåÎì§Ïùº Í≤ÉÏûÖÎãàÎã§. Í≤∞Íµ≠ Ïù¥ Î™®Îì† Íµ¨ÏÑ±ÏõêÏùÄ Í∞ôÏùÄ Î™©Ï†ÅÏùÑ Í∞ñÍ≥† Îç∞Ïù¥ÌÑ∞Î•º Îã§Î£®Îäî ÏÇ¨ÎûåÎì§Ïù¥ÎùºÎäî Í≥µÌÜµÏ†êÏù¥ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê, Ïûò ÏÑ§Ï†ïÎêú Î™©ÌëúÎ•º ÎÑìÍ≤å Ïù¥Ìï¥Ìïú ÏÉÅÌÉúÏó¨Ïïº ÌïòÍ≤†ÏäµÎãàÎã§.*\n\nÏù¥Î•º ÌÜµÌï¥ Îç∞Ïù¥ÌÑ∞ ÌååÏù¥ÌîÑÎùºÏù∏Ïù¥ Ïñ¥Îñ§ ÏãùÏúºÎ°ú Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏû•ÌïòÍ≥†, ÏóÖÎç∞Ïù¥Ìä∏ ÎêòÏñ¥Ïïº ÌïòÎäî ÏßÄÎ•º ÌååÏïÖÌï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n## ÎÇ¥Í∞Ä Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏïÑÌÇ§ÌÖçÏ≥êÎ•º ÎßåÎì†Îã§Î©¥,\nÏúÑÏóêÏÑú ÏÑ§Î™ÖÌñàÎìØÏù¥ Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Îäî Ïûò Í∞ñÏ∂∞ÏßÑ ÎèÑÎ©îÏù∏ ÏßÄÏãùÏùÑ Î∞îÌÉïÏúºÎ°ú Í∞ÄÏû• Î®ºÏ†Ä Îç∞Ïù¥ÌÑ∞Ïùò Ï†ÑÎ∞òÏ†ÅÏù∏ Ïù¥Ìï¥Î•º Í∞ñÏ∂îÍ≥†, ÏÜåÎπÑÏûêÏùò ÏàòÏöîÎ•º Ïù¥Ìï¥Ìïú Îã§ÏùåÏóê Ïù¥Ïóê Îî∞Îùº Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏïÑÌÇ§ÌÖçÏ≥êÎ•º ÏÑ§Í≥ÑÌï† Ïàò ÏûàÏñ¥Ïïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÏù¥ Í≥ºÏ†ïÏùÑ ÏÑ∏Ïã¨ÌïòÍ≤å Îî∞Î•¥ÏßÄ ÏïäÎäîÎã§Î©¥, ÎãπÏã†ÏùÄ Ïó¨Îü¨ Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ ÏàòÏöîÏóê ÎåÄÏùëÌïòÍ±∞ÎÇò, ÏàòÏãúÎ°ú Îç∞Ïù¥ÌÑ∞ Ïä§ÌÇ§ÎßàÎ•º Î≥ÄÍ≤ΩÌïòÎäî ÏûëÏóÖÏùÑ ÏßÑÌñâÌïòÎäî Îì±Ïùò Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏïÑÌÇ§ÌÖçÏ≥ê Î¶¨Ìå©ÌÜ†ÎßÅÏùÑ ÌïòÎäêÎùº Í∑ÄÌïú ÏãúÍ∞ÑÏùÑ ÏÜåÎ™®ÌïòÍ≤å Îê† Í≤ÉÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\nÏù¥Ïóê Îî∞Îùº, ÎßåÏùº Ï†úÍ∞Ä Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏïÑÌÇ§ÌÖçÏ≥êÎ•º Ï≤òÏùåÎ∂ÄÌÑ∞ ÏÑ§Í≥ÑÌïúÎã§Î©¥, Ïñ¥Îñ§ Í≤ÉÎì§ÏùÑ Í≥†Î†§ÌïòÍ≥†Ïûê ÌïòÎäî ÏßÄ ÏïÑÎûòÏóê Ï†ÅÏñ¥Î≥¥Î†§Í≥† Ìï©ÎãàÎã§.\n\n### 1. Í∞ÄÎä•Ìïú Ìïú ÏßÄÍ∏à Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞Îì§Ïóê ÎåÄÌï¥ ÏµúÎåÄÌïú Ïù¥Ìï¥Ìï©ÎãàÎã§.\nÏ±ÖÏÉÅ ÏúÑÏóê ÌéºÏ≥êÏßÑ Îç∞Ïù¥ÌÑ∞Îì§ÏùÑ Ï∂©Î∂ÑÌûà Ïù¥Ìï¥ÌïòÏßÄ Î™ªÌïú ÏÉÅÌÉúÏóêÏÑú ÏÑ§Í≥ÑÌïú Îç∞Ïù¥ÌÑ∞ Ïõ®Ïñ¥ÌïòÏö∞Ïä§ ÏïÑÌÇ§ÌÖçÏ≥êÎäî ÏàòÎ™ÖÏù¥ ÏßßÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. ÏúÑÏóêÏÑú ÎßêÌïú Í≤É Ï≤òÎüº Î≥¥Ïú†Ìïú Îç∞Ïù¥ÌÑ∞Î•º Ïù¥Ìï¥ÌïòÍ≥†, Ïù¥ Îç∞Ïù¥ÌÑ∞Ïùò ÏÜåÎπÑÏûêÎì§Í≥ºÏùò Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖòÏùÑ ÌÜµÌï¥ Îã¨ÏÑ±Ìï¥Ïïº ÌïòÎäî Î™©ÌëúÎ•º Ïù¥Ìï¥Ìï† ÏãúÍ∞ÑÏù¥ Íº≠ ÌïÑÏöîÌï©ÎãàÎã§.\n\n### 2. Í∞ÄÏû• Î®ºÏ†Ä Îã¨ÏÑ±Ìï¥Ïïº ÌïòÎäî Î™©ÌëúÏôÄ Í∑∏ Î™©ÌëúÎ•º Îã¨ÏÑ±Ìï† Î∞©Î≤ïÏùÑ ÌôïÏù∏Ìï©ÎãàÎã§.\nÏ†ÄÎäî Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏïÑÌÇ§ÌÖçÏ≥êÏóêÎäî ÎπÑÏ¶àÎãàÏä§ ÌÉÄÎãπÏÑ±Ïù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. Î™®Îì† Í∞ÄÎä•ÏÑ±ÏùÑ Í≥†Î†§Ìïú Í∞ÑÍ≤∞ÌïòÍ≥† Ïú†ÏßÄÎ≥¥ÏàòÌïòÍ∏∞ Ï¢ãÏùÄ ÏïÑÌÇ§ÌÖçÏ≥êÎäî ÌòÑÏã§Ïóê Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§. Í∞ÄÏû• Î®ºÏ†Ä Îã¨ÏÑ±Ìï¥Ïïº ÌïòÎäî Î™©ÌëúÎ•º Îã¨ÏÑ±Ìï† Ïàò ÏûàÎäî Î∞©Î≤ïÏùÑ Í≥†Î†§Ìï¥Ïïº Ìï©ÎãàÎã§. Í∑∏Î¶¨Í≥† Ïù¥ Í≥ºÏ†ïÏóêÏÑú ÏµúÎåÄÌïú ÏâΩÍ≥† Ï†ÅÏùÄ ÎπÑÏö©Ïùò ÌÖåÌÅ¨ Ïä§ÌéôÍ≥º Íµ¨ÌòÑ Î∞©ÏãùÏùÑ Ï±ÑÌÉùÌï¥Ïïº Ìï©ÎãàÎã§. ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅÏùÄ ÎãπÏó∞ÌïòÍ≤åÎèÑ Îçî Î≥µÏû°Ìï¥Ïßà Í≤ÉÏù¥Í∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n\nÏù¥ Í≥ºÏ†ïÏóêÏÑú Ï†úÍ∞Ä Í≥†Î†§ÌïòÍ≥†Ïûê ÌïòÎäî Í≤ÉÎì§ÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§.\n\n#### ÏÜåÎπÑÏûê Í¥ÄÏ†êÏóêÏÑúÏùò ÏöîÍµ¨ÏÇ¨Ìï≠:\nÎç∞Ïù¥ÌÑ∞ ÌååÏù¥ÌîÑÎùºÏù∏Ïùò Í≤∞Í≥ºÎ¨ºÏùÑ ÏÜåÎπÑÌï† Data Analyst, Backend EngineerÎì§Í≥º Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖòÏùÑ ÌÜµÌï¥ ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ ÌååÏïÖÌï¥Ïïº Ìï©ÎãàÎã§. ÌïÑÏàòÏ†ÅÏúºÎ°ú Ìè¨Ìï®ÎêòÏñ¥Ïïº ÌïòÎäî Ï†ïÎ≥¥ÏôÄ, Îç∞Ïù¥ÌÑ∞ ÌòïÏãù, Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ Ï£ºÍ∏∞Ïóê ÎåÄÌï¥ÏÑú Ìï©ÏùòÍ∞Ä Ïù¥Î§ÑÏ†∏Ïïº Ìï©ÎãàÎã§. Í∑∏Í≤ÉÏùÄ SLAÏùò ÌòïÌÉúÏùº ÏàòÎèÑ ÏûàÏäµÎãàÎã§. Îã§Îßå, Ïù¥ÏÉÅÏ†ÅÏúºÎ°ú ÏôÑÎ≤ΩÌïú Íµ¨ÌòÑÏù¥ ÏöîÍµ¨ÎêúÎã§Î©¥, ÎπÑÏ¶àÎãàÏä§ ÌÉÄÎãπÏÑ±ÏùÑ Í≥†Î†§Ìï¥ Îã¨ÏÑ± Í∞ÄÎä•Ìïú Î™©ÌëúÎ•º ÏÑ∏Î∂ÑÌôîÌï¥ÏÑú ÏùºÏ†ïÏùÑ ÏÇ∞Ï∂úÌïòÍ≥† ÏûëÏóÖÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n#### ÌÖåÌÅ¨ Ïä§Ìéô ÏÑ†Ï†ï:\nÍµ¨ÌòÑÏùÑ ÏúÑÌï¥ÏÑú ÌÖåÌÅ¨ Ïä§ÌéôÏùÑ ÏÑ†Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥ Îïå Í≥†Î†§Ìï¥Ïïº ÌïòÎäî Í≤ÉÏùÄ,\n\n1. Îç∞Ïù¥ÌÑ∞ Ïù∏ÌîÑÎùº ÏïÑÌÇ§ÌÖçÏ≥ê\n\n2. Îç∞Ïù¥ÌÑ∞ Ïù∏ÌîÑÎùºÏóê ÎåÄÌïú ÌåÄÏõêÎì§Ïùò Í∏∞Ïà† ÏπúÌôîÎèÑ\n\n3. Îç∞Ïù¥ÌÑ∞ Ïù∏ÌîÑÎùºÏóê ÎåÄÌïú Ï£º/Ïõî Îã®ÏúÑ ÎπÑÏö© ÏòàÏ∏°\n\n4. Íµ¨ÌòÑÏóê ÌïÑÏöîÌïú ÏãúÍ∞Ñ ÏòàÏ∏°Ïù¥ ÏûàÏäµÎãàÎã§.\n\nÎì±Ïù¥ ÏûàÏäµÎãàÎã§.\n\nÎ™®Îì† ÏöîÏÜåÍ∞Ä Î∂àÌôïÏã§ÏÑ±Ïù¥ ÎÜíÏúºÎ©¥ÏÑúÎèÑ Ï§ëÏöîÎèÑÏùò ÏàúÏÑúÎ•º Í≤∞Ï†ïÌïòÍ∏∞ Ïñ¥Î†µÎã§Îäî Î¨∏Ï†úÍ∞Ä ÏûàÏäµÎãàÎã§. ÌíÄÍ≥†Ïûê ÌïòÎäî Î¨∏Ï†úÍ∞Ä Î™®Îì† ÌöåÏÇ¨Ïùò Î™®Îì† ÌåÄÏóêÏÑú Í∞ÅÍ∏∞ Îã§Î•∏ Î™®ÏäµÏùÑ ÌïòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n\nÏ†ÄÎäî Ïù¥Îü∞ ÏÉÅÌô©ÏóêÏÑúÎäî Í∞ÄÏû• Î®ºÏ†Ä ÌåÄÏõêÎì§Í≥º Ìï®Íªò, Pre-MortemÏùÑ ÏßÑÌñâÌï¥Î≥¥Îäî Í≤ÉÎèÑ Ï¢ãÏùÄ Î∞©Î≤ïÏù¥ Îê† Ïàò ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÎåÄÎãàÏñº Ïπ¥ÎÑàÎ®ºÏùÄ Ï±Ö 'ÏÉùÍ∞ÅÏóê Í¥ÄÌïú ÏÉùÍ∞Å'ÏóêÏÑú Í≥ÑÌöç Ïò§Î•òÎùºÎäî Í∞úÎÖêÏùÑ Ïñ∏Í∏âÌïòÎ©∞, ÎßéÏùÄ ÏÇ¨ÎûåÎì§Ïù¥ Í≥ºÏã†Ïóê Îπ†ÏßÑ ÎÇòÎ®∏ÏßÄ Í≥ÑÌöçÏùÑ ÏÑ∏Ïö∏ Îïå, ÎπÑÌòÑÏã§Ï†ÅÏúºÎ°ú ÏµúÏÉÅÏùò ÏãúÎÇòÎ¶¨Ïò§Î•º ÏßúÍ±∞ÎÇò, ÎπÑÏä∑Ìïú Îã§Î•∏ ÏÉÅÌô©Ïùò ÏÇ¨Î°ÄÎ•º Ï∞æÏïÑÎ≥¥Îäî Ïô∏Î∂Ä Í¥ÄÏ†êÏùÑ Î¨¥ÏãúÌï¥ Í≥ÑÌöçÏùÑ ÏàòÌñâÌïòÎäî Í≤ÉÏóê Ïã§Ìå®ÌïúÎã§Í≥† ÎßêÌï©ÎãàÎã§.\nÏ†ÄÎäî Ïù¥Îü∞ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥, Pre-MortemÏùÑ ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏßÑÌñâÌï¥Î≥º Ïàò ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. Í∑∏Îäî ÏõêÏù∏Ïù¥ ÎêòÎäî ÏßÄÎÇòÏπú ÎÇôÍ¥ÄÏ£ºÏùòÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥, Pre-MortemÏùÑ Ï†úÏïàÌï©ÎãàÎã§. Ïù¥ Í∞úÎÖêÏùÑ Ïö∞Î¶¨ Î¨∏Ï†úÏóê Ï†ÅÏö©Ìï¥Î≥¥Î©¥, ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏßÑÌñâÌï¥Î≥º Ïàò ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\n1. ÌåÄÏõêÎì§Í≥º Î™®Ïó¨ÏÑú 1,3,6Í∞úÏõî Îí§Ïóê Ïù¥ ÌÖåÌÅ¨ Ïä§ÌéôÏù¥ Ï∞∏Îã¥ÌïòÍ≤å Ïã§Ìå®ÌïòÎäî Ïù¥Ïú†Îì§ÏùÑ Í∞ÅÏûê Ï†ÅÏñ¥Î≥∏Îã§.\n\n2. Í∞Å Ïã§Ìå® Ïù¥Ïú†Îì§Ïóê ÎåÄÌï¥ Í≥ÑÌöç Í≥ºÏ†ïÏóêÏÑú ÏòàÎ∞©Ìï† Î∞©Î≤ïÏùÑ ÌôïÏù∏ÌïúÎã§.\n\n3. Í∞Å Ïã§Ìå® Ïù¥Ïú†Îì§ÏùÑ Ìï¥Í≤∞Ìïú Ïô∏Î∂Ä ÏÇ¨Î°ÄÎ•º Í∞ÑÎã®Ìûà ÌÉêÏÉâÌïúÎã§.\n\nÏù¥Î†áÍ≤å ÎêòÎ©¥, Ìï¥Îãπ ÌåÄÏù¥ ÏßëÎã®Ï†ÅÏúºÎ°ú Í≤∞Ï†ïÏóê ÏàúÏùëÌïòÎäî Í≤ÉÏùÑ Î∞©ÏßÄÌï† Ïàò ÏûàÏúºÎ©∞, Î∞ïÏãùÌïú ÏÇ¨ÎûåÎì§Ïù¥ Î∞îÎûåÏßÅÌïú Î∞©Ìñ•ÏúºÎ°ú ÏÉÅÏÉÅÎ†•ÏùÑ ÌéºÏ≥êÎÇ¥ ÏúÑÌóò ÏöîÏÜåÎ•º ÌååÏïÖÌï¥Î≥º Ïàò ÏûàÎã§Îäî Ïû•Ï†êÏù¥ ÏûàÏäµÎãàÎã§.\nÏù¥ Í≥ºÏ†ïÏùÑ Í±∞Ïπú Îã§ÏùåÏóê, Îã§Ïãú ÏúÑÏùò 4Í∞ÄÏßÄ Í≥†Î†§ÏÇ¨Ìï≠ÏùÑ ÏµúÎåÄÌïú ÎßåÏ°±ÌïòÎäî ÌÖåÌÅ¨ Ïä§ÌéôÏùÑ ÏÑ†Ï†ïÌïúÎã§Î©¥, Ìõ®Ïî¨ Ïã†Î¢∞ÎèÑ ÎÜíÏùÄ ÌÖåÌÅ¨ Ïä§ÌéôÏùÑ ÏÑ†Ï†ïÌï† Ïàò ÏûàÏùÑ Í≤ÉÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\n### 3. Îç∞Ïù¥ÌÑ∞ StageÎ•º Í≥ÑÌöçÌï©ÎãàÎã§.\nÍ∞Å StageÎ≥Ñ Ïã§ÌñâÎêòÎäî ETL ÏûëÏóÖÏùò Î™©Ï†ÅÏùÑ Ï†ïÏùòÌïòÍ≥†, Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞Ïùò Ïó≠Ìï†ÏùÑ Ï†ïÏùòÌïòÎäî Í≥ºÏ†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.\n\nÏòàÎ•º Îì§Ïñ¥ data sourceÎ•º Ï†ÅÏû¨ÌïòÎäî data lakeÏôÄ, Í∞ÄÍ≥µÎêú Îç∞Ïù¥ÌÑ∞Îì§Ïù¥ Ìè¨Ìï®ÎêòÎäî data warehouse, Ïã§Ï†ú product ÎòêÎäî Î∂ÑÏÑùÏóê ÏÇ¨Ïö©Îê† data martÎ°ú stageÎ•º Íµ¨Î∂ÑÌï¥ Í≥ÑÌöçÌïòÎäî Î∞©Î≤ïÏù¥ ÏûàÏäµÎãàÎã§. Í∞Å stageÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨ÌïòÎäî pipeÏùò Ïó≠Ìï†Ïù¥ Î™ÖÌôïÌï†ÏàòÎ°ù Ï∂îÌõÑÏóê Íµ¨Ï°∞ Í∞úÏÑ†ÏùÑ Ìï† ÎïåÏóê Í∞úÏÑ† ÏßÄÏ†êÏùÑ ÌååÏïÖÌïòÍ∏∞ Ï¢ãÏùÑ Í≤ÉÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\nÍ∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏÑ∏Ïö¥ ÏõêÏπôÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§.\n\n1. Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Í∑úÏπô:\n\n    Í∞Å stageÎäî Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Í∑úÏπôÏùÑ Î™ÖÌôïÌïòÍ≤å Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥Î•ºÌÖåÎ©¥, Î™®Îì† stageÍ∞Ä non-volatileÌï† ÌïÑÏöîÎäî ÏóÜÏßÄÎßå, SourceÎäî Non-volatileÏùÑ Íº≠ ÏßÄÏºúÏïº ÌïúÎã§Îäî Í∑úÏπôÏùÑ ÏÑ∏Ïö∏ Ïàò ÏûàÏäµÎãàÎã§.\n\n2. Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò Í∑úÏπô:\n\n    Í∞Å stageÏùò Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò Í∑úÏπôÏùÑ Î™ÖÌôïÌïòÍ≤å Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥Î•ºÌÖåÎ©¥, data martÏùò Îç∞Ïù¥ÌÑ∞ ÏÑ±Í≤©ÏùÄ Ïó¨Îü¨ Í∞úÏùò data sourceÎ•º ÌÜµÌï¥ ÏÉàÎ°úÏö¥ Ï†ïÎ≥¥Í∞Ä ÏÉùÏÑ±Îêú Îç∞Ïù¥ÌÑ∞Ïùº ÏàòÎèÑ ÏûàÍ≥†, pipeline Ïô∏Î∂ÄÏùò ÏÇ¨Ïö©Ï≤òÍ∞Ä Î™ÖÌôïÌûà Ï†ïÌï¥ÏßÑ Îç∞Ïù¥ÌÑ∞Ïùº ÏàòÎèÑ ÏûàÏäµÎãàÎã§. ÏÑ±Í≤©ÏùÑ Î™ÖÌôïÌïòÍ≤å Î∂ÑÎ•òÌïòÏßÄ ÏïäÏïÑ data martÏóê Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï† Îç∞Ïù¥ÌÑ∞Í∞Ä Îã§Î•∏ stageÏóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÎäî Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌïòÏßÄ ÏïäÎèÑÎ°ù Ìï¥Ïïº Ìï©ÎãàÎã§.\n\n3. Îç∞Ïù¥ÌÑ∞ Í∞ÄÍ≥µ Í∑úÏπô:\n\n    Í∞Å stageÏùò Îç∞Ïù¥ÌÑ∞ Í∞ÄÍ≥µ Í∑úÏπôÏùÑ Î™ÖÌôïÌïòÍ≤å Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§. Ïù¥Î•ºÌÖåÎ©¥, data mart Ïù¥Ï†ÑÏùò stageÏùò Îç∞Ïù¥ÌÑ∞Î•º Îã§Î£®Îäî pipelineÏùÄ Î©±Îì±ÏÑ±ÏùÑ Î≥¥Ïû•ÌïòÎäî Î°úÏßÅÏùÑ Í∞ïÏ†úÌï† Ïàò ÏûàÏäµÎãàÎã§. ÎòêÎäî, data lakeÏùò Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÍ≥µÌï† ÎïåÏóêÎäî Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î≥ÄÍ≤Ω, ÏπºÎüº Ïù¥Î¶Ñ Î≥ÄÍ≤Ω Îì±Îßå ÌóàÏö©ÌïòÎäî Í∑úÏπôÏùÑ Í∞ïÏ†úÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n4. ÏòàÏô∏ Ï≤òÎ¶¨ Í∑úÏπô:\n\n    a,b,cÏóê Í¥ÄÌïú ÏòàÏô∏Îäî ÎπÑÏ¶àÎãàÏä§ ÌÉÄÎãπÏÑ±Ïóê ÎßûÍ≤å ÎåÄÏùëÌïòÎêò Î™®Îì† ÏòàÏô∏ ÏÇ¨Ìï≠ÏùÄ re-architecturing pointÎ°ú Í∏∞Î°ùÌï¥ revisit Í∞ÄÎä•ÌïòÍ≤å Ìï¥Ïïº Ìï©ÎãàÎã§.\n\nÏù¥ Í∑úÏπôÎì§ Ï§ëÏóêÏÑú Í∞ÄÏû• ÍπåÎã§Î°úÏö¥ ÏßÄÏ†êÏùÄ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ•ò Í∑úÏπôÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥ data martÏùò ÌÖåÏù¥Î∏îÎì§ÏùÄ Í∑∏ÎåÄÎ°ú ProductÏóê ÏÇ¨Ïö©ÎêòÍ∏∞ÎèÑ ÌïòÏßÄÎßå, mart ÌÖåÏù¥Î∏î Ïó¨Îü¨ Í∞úÎ•º joinÌï¥ÏÑú ÏÉàÎ°úÏö¥ ÌÖåÏù¥Î∏îÎ°ú ÎßåÎì§Ïñ¥ Î∂ÑÏÑùÏóê ÌôúÏö©Ìï† Ïàò ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÏù¥Îü¨Ìïú Í≤ΩÏö∞Ïóê ÎåÄÏùëÌïòÍ∏∞ ÏúÑÌï¥ÏÑú Ïö∞Î¶¨Îäî Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌåêÎã®ÏùÑ Ìï† Ïàò ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\n1. sourceÎì§ÏùÑ ÏµúÎåÄÌïú ÌôúÏö©ÌïòÏó¨ Î∂ÑÏÑù Î™©Ï†Å Ïö©ÎèÑÎ°ú ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÇ∞ Í∞ÄÎä•Ìïú ÏßÄÎ•º Í≥†Î†§Ìï¥Î≥º Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n2. Î∂ÑÏÑù Î™©Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©Ìï† data mart ÌÖåÏù¥Î∏îÏùÑ sourceÎ°ú ÌïòÎäî Î∂ÑÏÑù Î™©Ï†ÅÏùò data warehouseÎ•º ÏÉàÎ°≠Í≤å Íµ¨Ï∂ïÌï† Ïàò ÏûàÎäî ÏßÄ Í≥†Î†§Ìï¥Î≥º Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\nÏù¥ Ïô∏ÏóêÎèÑ ÎπÑÏ¶àÎãàÏä§ ÏÉÅÌô©Ïóê Îî∞Îùº Îã§Î•∏ Í≥†Î†§Ìï† Ïàò ÏûàÎäî ÏòµÏÖòÎì§Ïù¥ ÏûàÍ≤†ÏäµÎãàÎã§. ÎßéÏùÄ Í≤ΩÌóòÍ≥º Í≥µÎ∂ÄÎ•º ÌÜµÌï¥ Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Í∞Ä ÏÑ±Ïû•ÌïòÎäî ÏßÄÏ†êÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\n### 4. Îç∞Ïù¥ÌÑ∞ Lineage Í¥ÄÎ¶¨ Î∞©Î≤ïÏùÑ Í≥ÑÌöçÌï©ÎãàÎã§.\nÎπÑÏ¶àÎãàÏä§ Î°úÏßÅÏù¥ Î≥µÏû°Ìï¥ÏßàÏàòÎ°ù, ÏµúÏ¥àÏóê Îç∞Ïù¥ÌÑ∞ ÏïÑÌÇ§ÌÖçÏ≥êÎ•º Í≥ÑÌöçÌïú ÏûëÏóÖÏûêÎì§ÏúºÎ°ú Ïù∏Ìï¥ single point of failureÍ∞Ä Î∞úÏÉùÌï† ÌôïÎ•†Ïù¥ ÎÜíÏïÑÏßÑÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. Îî∞ÎùºÏÑú ÎàÑÍµ¨ÎÇò ÌïôÏäµÌï† Ïàò ÏûàÎäî Lineage Í¥ÄÎ¶¨ Î∞©Î≤ïÏùÑ Í≥†ÎØºÌï¥Ïïº Ìï©ÎãàÎã§. ÎàÑÍµ¨ÎÇò ÌïôÏäµÌïòÍ∏∞ÏóêÎäî Ïñ¥Î†§Ïö¥ Íµ¨Ï°∞ÎùºÍ≥† ÏÉùÍ∞ÅÏù¥ Îì§ÏóàÎã§Î©¥, Í≤∞Íµ≠ Ïù¥Îäî SPFÎ•º Î∞úÏÉùÏãúÌÇ§Í≤å Îê† Í≤ÉÏûÖÎãàÎã§.\n\nÎç∞Ïù¥ÌÑ∞ LineageÍ∞Ä ÏÉùÏÇ∞ÏÑ±ÏùÑ Ìï¥ÏπòÏßÄ ÏïäÎäî ÏÑ†ÏóêÏÑú Íµ¨ÏÑ±ÎêòÎ†§Î©¥ ÏïÑÎûòÏùò ÏÉÅÌô©ÏùÄ ÏµúÎåÄÌïú ÌîºÌïòÎäî Í≤ÉÏù¥ Ï¢ãÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\n1. Ìïú data warehouse ÎÇ¥ÏóêÏÑú cyclic Íµ¨Ï°∞Í∞Ä ÏÉùÍ≤®Ïïº ÌïòÎäî Í≤ΩÏö∞\n\n2. ÏÑúÎ°ú Îã§Î•∏ data stageÏóê Ìè¨Ìï®Îêú sourceÎ•º Ï∞∏Ï°∞Ìï¥ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÍ≥µÌïòÎäî Í≤ΩÏö∞\n\nÏúÑ Îëê Í∞ÄÏßÄ ÏÉÅÌô©ÏùÄ 3Î≤àÏóêÏÑú Í≥ÑÌöçÌïú Îç∞Ïù¥ÌÑ∞ stageÏùò Ï†ïÏùòÎ•º Ìï¥ÏπòÍ≤å Îê©ÎãàÎã§.\nÏù¥Îü¨Ìïú Î¨∏Ï†úÎì§ÏùÄ ÎèÑÎ©îÏù∏ ÏßÄÏãùÍ≥º ÎπÑÏ¶àÎãàÏä§ Î™©Ìëú, Ïù¥Ìï¥ÎãπÏÇ¨ÏûêÎì§Í≥ºÏùò Ïª§ÎÆ§ÎãàÏºÄÏù¥ÏÖòÏùÑ Ïûò ÌôúÏö©Ìï¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞Ìï¥Ïïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\nÎ≥µÏû°ÎèÑÍ∞Ä ÎÜíÏùÄ Îç∞Ïù¥ÌÑ∞ lineageÎäî Í≤∞Íµ≠ ÎÇòÏ§ëÏóê Îç∞Ïù¥ÌÑ∞ ÏÉùÏÇ∞ÏÑ±Ïùò Î≥ëÎ™© ÏöîÏù∏Ïù¥ Îê† Í≤ÉÏù¥Í∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n\nÏ†ÄÎäî Ïó¨Í∏∞ÏóêÏÑú Íº≠ ÌîºÌï¥Ïïº ÌïòÎäî ÏÑ†ÌÉùÎì§Ïù¥ ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n\n1. Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÍ∞Ä ÎòêÎäî Îç∞Ïù¥ÌÑ∞ Î∞±ÏóîÎìú ÏóîÏßÄÎãàÏñ¥Ïùò Ï≤òÎ¶¨ Î∞©ÏãùÏùÑ Î≥ÄÍ≤ΩÌïòÎäî Ìï¥Í≤∞Ï±Ö:\n\n    Ïù¥ Ìï¥Í≤∞Ï±ÖÏùÄ Í≤∞Íµ≠ Îã§Ïãú Í∞ôÏùÄ Î¨∏Ï†úÎ•º ÏßÅÎ©¥ÌïòÍ≤å Ìï† Í≤ÉÏûÖÎãàÎã§.\n\n2. Î∂àÍ∞ÄÌîºÌïòÍ≤å Î≥µÏû°ÎèÑÎ•º ÏÑ†ÌÉùÌïú Í≤ΩÏö∞Ïóê, Ìï¥Îãπ ÏÑ†ÌÉùÏùò Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞Î•º Ïù¥ÌõÑ Îç∞Ïù¥ÌÑ∞Ïùò sourceÎ°ú ÌôúÏö©ÌïòÎäî Í≤ΩÏö∞:\n\n    Î≥µÏû°ÎèÑÎ•º ÌîºÌï† Ïàò ÏóÜÎäî Íµ¨Ï°∞ÎùºÎäî Ïù¥Ïú†Î°ú ÏòàÏô∏ ÌóàÏö©ÏùÑ ÌÉùÌñàÎã§Î©¥, ÌóàÏö©Îêú ÏòàÏô∏Í∞Ä downstreamÏóêÏÑú Î≥µÏû°ÎèÑÎ•º ÎÜíÏù¥ÏßÄ Î™ªÌïòÍ≤å ÌÜµÏ†úÌï¥Ïïº Ìï©ÎãàÎã§.\n\n### 5. Îç∞Ïù¥ÌÑ∞ Validation, Verification \u0026 Monitoring Î∞©Î≤ïÏùÑ Íµ¨ÏÉÅÌï©ÎãàÎã§.\nÏ¢ãÏùÄ Îç∞Ïù¥ÌÑ∞ ÏïÑÌÇ§ÌÖçÏ≥êÎ•º ÏÑ§Í≥ÑÌñàÎã§Í≥† ÌïòÎçîÎùºÎèÑ, Îäò Ïö∞Î¶¨Îäî ÏòàÏÉÅÏπò Î™ªÌïú ÏòàÏô∏ Îç∞Ïù¥ÌÑ∞Î•º ÎßàÏ£ºÌï¥Ïïº Ìï©ÎãàÎã§.\nÏù¥Îü¨Ìïú ÏòàÏô∏Îäî Ìï†ÎãπÎêú Î¶¨ÏÜåÏä§ Î≤îÏúÑÎ•º Ï¥àÍ≥ºÌïòÎäî Î≥µÏû°Ìïú Î°úÏßÅ ÎïåÎ¨∏Ïóê Î∞úÏÉùÌï† ÏàòÎèÑ ÏûàÍ≥†, ÎπÑÏ¶àÎãàÏä§ ÏûÑÌå©Ìä∏Î•º Ï£ºÏßÄ Î™ªÌïòÎäî outlierÏóê Ìï¥ÎãπÌïòÎäî InputÏúºÎ°ú Ïù∏Ìï¥ Î∞úÏÉùÌï† ÏàòÎèÑ ÏûàÏúºÎ©∞, Îç∞Ïù¥ÌÑ∞ sourceÏùò Ïä§ÌÇ§Îßà Î≥ÄÍ≤Ω ÎòêÎäî Î≤ÑÏ†Ñ ÏóÖÍ∑∏Î†àÏù¥ÎìúÎ°ú Ïù∏Ìï¥ Î∞úÏÉùÌï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.\n\nÎç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥ÏóêÍ≤å Ïñ¥Îñ§ Í≤ÉÎ≥¥Îã§ Ï§ëÏöîÌïú Í≤ÉÏùÄ Îç∞Ïù¥ÌÑ∞ ÌÄÑÎ¶¨Ìã∞ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. Í∑∏Î†áÍ∏∞ ÎïåÎ¨∏Ïóê Îç∞Ïù¥ÌÑ∞ ÏóîÏßÄÎãàÏñ¥Îäî Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º Îã§Î£®Îäî ÌååÏù¥ÌîÑÎùºÏù∏Îì§Ïóê ÎåÄÌï¥ÏÑú Í∞ÄÎä•Ìïú ÏòàÏô∏Î•º Í∞ÄÏû• Î®ºÏ†Ä ÌååÏïÖÌïòÍ≥† ÎåÄÏùëÌï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\nÎî∞ÎùºÏÑú Ï¢ãÏùÄ Îç∞Ïù¥ÌÑ∞ ÏïÑÌÇ§ÌÖçÏ≥êÎ•º ÏôÑÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî ValidationÍ≥º VerificationÏùÑ Ïñ¥ÎñªÍ≤å Íµ¨ÏÑ±Ìï† ÏßÄ, Í∑∏Î¶¨Í≥† Ïù¥Î•º Ïñ¥ÎñªÍ≤å MonitoringÌï† ÏßÄ Í≤∞Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§.\n\nÏ†úÍ∞Ä Validation / VerificationÏóê ÎåÄÌï¥ÏÑú Ï§ëÏöîÌïòÍ≤å ÏÉùÍ∞ÅÌïòÎäî Í≤ÉÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§.\n\n1. StageÏùò Ï†ïÏùòÎ•º Îî∞Î•¥Îäî ÏßÄ ÌôïÏù∏Ìï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n2. data lineageÏóê Îî∞Îùº, ValidationÏùÑ ÌÜµÌï¥ ÌäπÏ†ï pointÏóê ÎåÄÌï¥ Í∞Å ValidationÏù¥ Îç∞Ïù¥ÌÑ∞Î•º Verification Ìï† Ïàò ÏûàÍ≤åÎÅî Î≥¥Ïû•Ìï¥Ïïº Ìï©ÎãàÎã§. ÎèôÏùºÌïú Î™©Ï†ÅÏùò ValidationÏù¥ Ïó¨Îü¨ stageÏóêÏÑú ÏàòÌñâÎê† ÌïÑÏöîÍ∞Ä ÏóÜÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n3. Validation Í≥ºÏ†ïÏóêÏÑú ÌååÏïÖÎêú ÏòàÏô∏ Îç∞Ïù¥ÌÑ∞Îì§ÏùÄ Î≥ÑÎèÑÎ°ú Í∏∞Î°ùÎêòÏñ¥, Verification RuleÏóê ÌôúÏö©ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n4. MonitoringÏùÄ Í∞úÎ∞ú Í≥ºÏ†ïÏóêÏÑú Í∞ÄÏû• ÏòàÏô∏Í∞Ä ÎßéÏù¥ Î∞úÏÉùÌïú ÏßÄÏ†êÏùÑ ÌååÏïÖÌï¥ÏÑú ÏßÑÌñâÌïòÎêò, Ïö¥ÏòÅ Í≥ºÏ†ïÏóêÏÑú Î∞úÏÉùÌïòÎäî IssueÎì§ÏùÑ ÌÜµÌï¥ Grey AreaÍ∞Ä Î∞úÏÉùÌïòÏßÄ ÏïäÎèÑÎ°ù Î∞úÏ†ÑÏãúÏºúÏïº Ìï©ÎãàÎã§.\n\n## Í≤∞Î°†\nÏ¢ãÏùÄ Îç∞Ïù¥ÌÑ∞ ÏïÑÌÇ§ÌÖçÏ≥êÏóê ÎåÄÌïú Ï†ïÎãµÏùÄ ÏóÜÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. Í≥ÑÏÜçÌï¥ÏÑú Îçî Ï¢ãÏùÄ Í∏∞Ïà†Îì§Ïù¥ ÏÑ∏ÏÉÅÏóê ÏÜåÍ∞úÎêòÍ≥† ÏûàÍ≥†, Îçî Ï∞ΩÏùòÏ†ÅÏù∏ Ìï¥Í≤∞Ï±ÖÏù¥ Ï†úÏãúÎê† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§. Ï†ÄÎäî Í∑∏ Ï†ïÎãµÏùÑ ÎßåÎì§Ïñ¥ÎÇ¥Îäî Í≥ºÏ†ïÏóêÏÑú ÎÜìÏπòÏßÄ ÏïäÏïÑÏïº ÌïòÎäî Í≤ÉÎì§Ïóê ÎåÄÌï¥ÏÑú Îäò Ï£ºÏùòÌï¥Ïïº ÌïúÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. Îã¨ÏÑ±Ìïú Í≤∞Í≥ºÎ¨ºÏù¥ ÏµúÏÑ†Ïù¥ ÏïÑÎãàÎùº Second BestÏùº ÏàòÎèÑ ÏûàÏäµÎãàÎã§. Í∑∏Î†áÏßÄÎßå WorstÍ∞Ä ÏïÑÎãå Best PracticeÎ•º ÏßÄÌñ•ÌïòÍ≥† Ïù¥Î•º ÏúÑÌï¥ Í≥†Î†§Ìï† ÏÇ¨Ìï≠Îì§ÏùÑ ÏûäÏßÄ ÏïäÏïÑÏïº Another BestÎ•º ÎßåÎì§Ïñ¥ ÎÇº Ïàò ÏûàÎã§Í≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/pandas_as_an_sql_en":{"title":"[DE] Pandas As an SQL (EN)","content":"\n\n## Pandas as an SQL\nDate : June 20, 2023\n\nI studied PANDAS as an SQL Statement\n```sql\nSELECT\n    table.a,\n    table.b as b1,\n    table2.c as c1,\n    table2.d,\n    sum(table2.e) as e1\nFROM table\nLEFT JOIN table2\nON table.a = table2.a\nWHERE true\n  AND table.a \u003e 0\n  AND (table.b = \"None\" or table2.c \u003e 0)\nGROUP BY table.a, table.b, table2.c, table2.d\nHAVING f1 \u003e 0\nORDER BY table.a ASC, table.b DESC, table2.c ASC\nLIMIT 100\nOFFSET 10\n```\nIf you were to write a query above in Pandas, you would write it like a code below:\n\n```python\nimport pandas as pd\n\ntable = pd.DataFrame(\n    {\n        'a': [1,2,3],\n        'b': [\"4\",\"4\",\"6\"],\n    }\n)\n\ntable2 = pd.DataFrame(\n        {\n                'a': [1,2,3],\n                'c': [1,2,3],\n                'd': [\"4\",\"6\",\"4\"],\n                'e': [1,2,3],\n        }\n)\n\nresult = (\n    table.merge(table2, on='a', how='left') # Join\n        [['a', 'b', 'c', 'd', 'e'] ] # Select\n        .rename(columns={'b': 'b1', 'c': 'c1'}) # Alias\n        .query('a \u003e 0 \u0026 (b1 == \"None\" | c1 \u003e 0)') # Where\n        .groupby(['a', 'b1', 'c1', 'd', 'e']) # Group by\n        .agg(\n                e1 = ('e', 'sum')\n        ) # Aggregation\n        .reset_index() # Reset Index - without this, the dataframe has multiple index consisted of columns which were used in group by statement\n        .query('e1 \u003e= 0') # Having\n        .sort_values(['a', 'b1', 'c1'], ascending=[True, False, True]) # Order by\n        .iloc[10:110] # limit and offset\n)\n\n```\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/pandas_as_an_sql_kr":{"title":"[DE] Pandas As an SQL (KR)","content":"\n\n## SQLÏ≤òÎüº Ïì∞Îäî Pandas\nÏûëÏÑ± ÏùºÏûê : 2023ÎÖÑ 6Ïõî 20Ïùº\n\nPandasÎ•º SQLÎ¨∏Ï≤òÎüº Í≥µÎ∂ÄÌï¥Î¥§ÏäµÎãàÎã§.\n```sql\nSELECT\n    table.a,\n    table.b as b1,\n    table2.c as c1,\n    table2.d,\n    sum(table2.e) as e1\nFROM table\nLEFT JOIN table2\nON table.a = table2.a\nWHERE true\n  AND table.a \u003e 0\n  AND (table.b = \"None\" or table2.c \u003e 0)\nGROUP BY table.a, table.b, table2.c, table2.d\nHAVING f1 \u003e 0\nORDER BY table.a ASC, table.b DESC, table2.c ASC\nLIMIT 100\nOFFSET 10\n```\nÏù¥ÎùºÎäî ÏøºÎ¶¨Î•º PandasÎ°ú ÏûëÏÑ±ÌïúÎã§Î©¥, ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏΩîÎìúÎ•º ÏûëÏÑ±Ìï©ÎãàÎã§.\n\n```python\nimport pandas as pd\n\ntable = pd.DataFrame(\n    {\n        'a': [1,2,3],\n        'b': [\"4\",\"4\",\"6\"],\n    }\n)\n\ntable2 = pd.DataFrame(\n        {\n                'a': [1,2,3],\n                'c': [1,2,3],\n                'd': [\"4\",\"6\",\"4\"],\n                'e': [1,2,3],\n        }\n)\n\nresult = (\n    table.merge(table2, on='a', how='left') # Join\n        [['a', 'b', 'c', 'd', 'e'] ] # Select\n        .rename(columns={'b': 'b1', 'c': 'c1'}) # Alias\n        .query('a \u003e 0 \u0026 (b1 == \"None\" | c1 \u003e 0)') # Where\n        .groupby(['a', 'b1', 'c1', 'd', 'e']) # Group by\n        .agg(\n                e1 = ('e', 'sum')\n        ) # Aggregation\n        .reset_index() # Reset Index - without this, the dataframe has multiple index consisted of columns which were used in group by statement\n        .query('e1 \u003e= 0') # Having\n        .sort_values(['a', 'b1', 'c1'], ascending=[True, False, True]) # Order by\n        .iloc[10:110] # limit and offset\n)\n\n```\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/dune_nft/market_overview_en":{"title":"[Dune] NFT Market Overview Chart Review (EN)","content":"\n\n## Market Overview Chart Review\nDate : March 22, 2023\n\nReviewed [NFT Market Overview](https://dune.com/hildobby/NFTs)\n\n### 1. Overview\n\n![Screenshot](/notes/dune_nft/images/market_overview/01_overview.png)\n\nYou can see that we have constructed a pie chart using **Volume / Number of trades / Number of traders**.\n\nGiven that the amount of data is large by default, and that more recent data is more meaningful for trading, they show the most recent 1-week data.\nThe trade column in the **NFT Collection Ranked by Volume** chart below are links to actual tradeable websites, so they don't mean much.\n\n### 2. Volume\n\n![Screenshot](/notes/dune_nft/images/market_overview/02_volume.png)\n\nTrading volume is organized into Bar Chart and Stacked Area Chart, each of which allows you to view Daily Volume and Weekly Volume.\n\nThe Bar Chart is a great way to see total volume, so you can see overall volume trends and time series of marketplace share.\nThe Stacked Area Chart is an intuitive representation of the share seen in this Bar Chart.\n\n\n\n### 3. Transactions\n\n![Screenshot](/notes/dune_nft/images/market_overview/03_transactions.png)\n\nThe transactions chart is organized the same as the volume chart, with the numbers in each chart representing the number of transactions.\n\nIf you compare the number of trades chart to the volume chart, you can see that Blur's volume has recently spiked and surpassed Opensea's volume, but that hasn't translated to the number of trades. We can assume that there are still a lot of active users on Opensea.\n\n### 4. Traders\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_traders.png)\n\nThis chart shows the number of wallets that have traded NFTs.\n\nIn this case, They used a Line Chart instead of a Stacked Area Chart. I thought it's better to use a Stacked Area Chart, so I made Stacked Area Chart for comparison\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_1_traders_area.png)\n\nI was able to see these results, and there are a few takeaways from this comparison.\n\n1. Stacked Area Chart has the advantage of visualizing occupancy rates.\n2. Line charts allow you to visualize the share, but also to see how the value changes over time. For example, you can observe a sharp drop in the number of traders in March.\n3. However, the Traders chart is different from the other charts in that the Bar Chart counts the number of trading wallets by date, while the Line Chart counts the number of trading wallets by marketplace. Since there may be wallets that use multiple marketplaces at the same time, it is important to keep this in mind when organizing the chart.\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_2_traders_area_2.png)\nIf you compare the screenshot above with the Bar Chart in the Dashboard, you can see that the values on the y-axis are different.\n\n### 5. Token Standards\n\n![Screenshot](/notes/dune_nft/images/market_overview/05_token_standards.png)\n\nThese Charts show Volume, Trade Count, and Trader Count by Token Standard.\n\nThis is where the question arose.\nIn the case of ERC721, you can only trade one token per token id, whereas ERC1155 allows you to trade multiple.\nAlso, Opensea allows bundle trade, so I was wondering how to calculate the price of nfts in these situation.\n\nSo I looked into Dune Spellbook. [ÎßÅÌÅ¨](https://dune.com/spellbook#!/model/model.spellbook.seaport_v2_ethereum_trades)\n![Screenshot](/notes/dune_nft/images/market_overview/06_spellbook_opensea.png)\n\nAs you can see in the screenshot above, if multiple NFTs were traded in a particular trade, we were able to categorize it as a bundle trade and then split the trade price equally.\nThis tells us the following:\n\n- The opensea trades included in the nft trades dataset used by Dune are calculated by dividing the total price of the individual purchases of nft in a trade by the number of nft in the trade.\n- As a result, some transaction value data was calculated incorrectly.\n- However, in bundle transactions, the seller estimates the individual prices of the NFTs in the bundle and sells them together, and the same applies to the buyer, so there is no problem with using this data unless there is an accounting requirement for both the seller and buyer.\n\n## Further Study\n\nWhile researching Spellbook, something occurred to me that I wanted to investigate further.\nI've seen that it's difficult to estimate the exact purchase price for bundle transactions on Opensea.\nI want to see how Dune Analytics estimate the purchase price for transactions from market aggregators like Gem and Blur that sweep listings from multiple marketplaces\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/notes/dune_nft/market_overview_kr":{"title":"[Dune] NFT Market Overview Chart Review (KR)","content":"\n\n## Market Overview Ï∞®Ìä∏ Î¶¨Î∑∞\nÏûëÏÑ± ÏùºÏûê : 2023ÎÖÑ 3Ïõî 22Ïùº\n\n[NFT Market Overview](https://dune.com/hildobby/NFTs)Î•º Î¶¨Î∑∞Ìï¥Î¥§ÏäµÎãàÎã§.\n\n### 1. Overview\n\n![Screenshot](/notes/dune_nft/images/market_overview/01_overview.png)\n\n**Í±∞ÎûòÎüâ / Í±∞Îûò ÌöüÏàò / Í±∞ÎûòÏûê Ïàò**Î•º Ïù¥Ïö©Ìï¥ ÌååÏù¥Ï∞®Ìä∏Î•º Íµ¨ÏÑ±Ìïú Í≤ÉÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\nÎç∞Ïù¥ÌÑ∞ ÏñëÏù¥ Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÎßéÍ≥†, ÏµúÏã† Îç∞Ïù¥ÌÑ∞Í∞Ä TradingÏóê Ï¢Ä Îçî Ïú†ÏùòÎØ∏ÌïòÎã§Îäî Ï†êÏùÑ Í∞êÏïàÌï¥, ÏµúÍ∑º 1Ï£º Îç∞Ïù¥ÌÑ∞Î•º Î≥¥Ïó¨Ï§çÎãàÎã§.\nÌïòÎã® **NFT Collection Ranked by Volume** Ï∞®Ìä∏Ïùò TradeÎäî Ïã§Ï†ú trade Í∞ÄÎä•Ìïú ÏõπÏÇ¨Ïù¥Ìä∏Ïùò ÎßÅÌÅ¨ÎùºÏÑú ÌÅ∞ ÏùòÎØ∏Îäî ÏóÜÏäµÎãàÎã§.\n\n### 2. Volume\n\n![Screenshot](/notes/dune_nft/images/market_overview/02_volume.png)\n\nÍ±∞ÎûòÎüâÏùÄ Bar ChartÏôÄ Stacked Area ChartÎ°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÍ≥†, Í∞ÅÍ∞ÅÏùÄ Daily VolumeÍ≥º Weekly VolumeÏùÑ Î≥º Ïàò ÏûàÏäµÎãàÎã§.\n\nBar ChartÎäî Í±∞ÎûòÎüâ Ï¥ùÌï©ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÎã§Îäî Ï†êÏóêÏÑú Ï†ÑÎ∞òÏ†ÅÏù∏ Í±∞ÎûòÎüâ Ï∂îÏù¥ÏôÄ ÏãúÍ≥ÑÏó¥Î°ú Marketplace Î≥Ñ Ï†êÏú†Ïú®ÏùÑ Í∞ÄÎä†Ìï† Ïàò ÏûàÏäµÎãàÎã§.\nStacked Area ChartÎäî Ïù¥ Bar ChartÏóêÏÑú ÌôïÏù∏Ìïú Ï†êÏú†Ïú®ÏùÑ ÏßÅÍ¥ÄÏ†ÅÏúºÎ°ú ÌëúÌòÑÌïú Í≤ÉÏûÖÎãàÎã§.\n\n### 3. Transactions\n\n![Screenshot](/notes/dune_nft/images/market_overview/03_transactions.png)\n\nÍ±∞Îûò ÌöüÏàòÎäî Í±∞ÎûòÎüâ Ï∞®Ìä∏ÏôÄ ÎèôÏùºÌïòÍ≤å Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÍ≥†, Í∞Å Ï∞®Ìä∏Ïùò ÏàòÏπòÎäî Í±∞Îûò ÌöüÏàòÎ•º ÏùòÎØ∏Ìï©ÎãàÎã§.\n\nÍ±∞Îûò ÌöüÏàò Ï∞®Ìä∏Î•º Í±∞ÎûòÎüâ Ï∞®Ìä∏ÏôÄ ÎπÑÍµêÌï¥ÏÑú Î≥¥Î©¥, ÏµúÍ∑º BlurÏùò Í±∞ÎûòÎüâÏù¥ Í∏âÎì±Ìï¥ OpenseaÏùò Í±∞ÎûòÎüâÏùÑ ÌÅ¨Í≤å ÎÑòÏñ¥ÏÑ∞ÏßÄÎßå, Í∑∏Í≤ÉÏù¥ Í±∞Îûò ÌöüÏàòÍπåÏßÄ Ïù¥Ïñ¥ÏßÄÏßÄÎäî ÏïäÏùÄ Í≤ÉÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§. ÏïÑÏßÅ OpenseaÏùò ÌôúÏÑ±Ìôî Ïú†Ï†ÄÍ∞Ä ÎßéÏù¥ ÎÇ®ÏïÑÏûàÏùåÏùÑ Ï∂îÏ∏°Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n### 4. Traders\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_traders.png)\n\nÏù¥ ChartÎäî NFTÎ•º Í±∞ÎûòÌïú ÏßÄÍ∞ëÏùò Ïàò Ï∂îÏù¥Î•º Î≥¥Ïó¨Ï§çÎãàÎã§.\n\nÏù¥ Í≤ΩÏö∞ÏóêÎäî Stacked Area Chart ÎåÄÏã†, Line ChartÎ•º ÏÇ¨Ïö©ÌñàÎäîÎç∞, Stacked Area ChartÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Îçî Ï¢ãÎã§Îäî ÏÉùÍ∞ÅÏù¥ Îì§Ïñ¥ ÎπÑÍµêÌï¥ Î≥¥Í≥†Ïûê ÏßÅÏ†ë ChartÎ•º ÏûëÏÑ±Ìï¥ Î¥§ÏäµÎãàÎã§.\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_1_traders_area.png)\n\nÏù¥Îü¨Ìïú Í≤∞Í≥ºÎ•º Î≥º Ïàò ÏûàÏóàÎäîÎç∞, Ïù¥ Ï∞®Ìä∏ÏôÄ Í∏∞Ï°¥ Ï∞®Ìä∏Î•º ÎπÑÍµêÌï¥ Î≥¥Í≥† Î™á Í∞ÄÏßÄ takeawayÎ•º ÏñªÏùÑ Ïàò ÏûàÏóàÏäµÎãàÎã§.\n1. Stacked Area ChartÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ Ï†êÏú†Ïú®ÏùÑ ÏãúÍ∞ÅÌôî ÌïòÎäî Í≤ÉÏóê Ïû•Ï†êÏù¥ ÏûàÎã§.\n2. Line ChartÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ Ï†êÏú†Ïú®ÏùÑ ÏãúÍ∞ÅÌôîÌïòÎäî ÎèôÏãúÏóê, ÏãúÍ≥ÑÏó¥Ïóê Îî∞Î•∏ Í∞í Î≥ÄÌôî Ï∞®Ïù¥Î•º Ïûò Î≥º Ïàò ÏûàÎã§. ÏòàÎ•º Îì§Ïñ¥, 3ÏõîÏóê Í∏âÍ≤©Ìûà Í±∞ÎûòÏûê ÏàòÍ∞Ä Í∏âÍ∞êÌïú ÏãúÍ∏∞Î•º Í¥ÄÏ∞∞ÌïòÎäî Í≤ÉÏù¥ Í∞ÄÎä•ÌïòÎã§.\n3. Îã§Îßå Traders Ï∞®Ìä∏Îäî Îã§Î•∏ Ï∞®Ìä∏ÏôÄ Îã¨Î¶¨ Ï£ºÏùòÌï† Ï†êÏù¥ ÏûàÏóàÎäîÎç∞, Bar ChartÏùò Í≤ΩÏö∞Îäî ÎÇ†ÏßúÎ≥Ñ Í±∞Îûò ÏßÄÍ∞ë ÏàòÎ•º ÏÑº Î∞òÎ©¥, Line ChartÏóêÏÑúÎäî Marketplace Î≥ÑÎ°ú Í±∞Îûò ÏßÄÍ∞ë ÏàòÎ•º ÏÑ∏Í≥† ÏûàÎã§. Ïó¨Îü¨ MarketplaceÎ•º ÎèôÏãúÏóê Ïù¥Ïö©Ìïú ÏßÄÍ∞ëÏù¥ Ï°¥Ïû¨Ìï† Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê, Ï∞®Ìä∏ Íµ¨ÏÑ± Ïãú Ïú†ÏùòÌïòÎ©¥ Ï¢ãÎã§.\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_2_traders_area_2.png)\nÏúÑ Ïä§ÌÅ¨Î¶∞ÏÉ∑Í≥º DashboardÏùò Bar ChartÎ•º ÎπÑÍµêÌï¥Î≥¥Î©¥ yÏ∂ïÏùò Í∞íÏù¥ Îã§Î•∏ Í≤ÉÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§.\n\n### 5. Token Standards\n\n![Screenshot](/notes/dune_nft/images/market_overview/05_token_standards.png)\n\nToken Standard Î≥ÑÎ°ú Volume, Trade Count, Trader CountÎ•º ÌëúÍ∏∞Ìïú Í≤ÉÏûÖÎãàÎã§.\n\nÏó¨Í∏∞ÏóêÏÑú Í∂ÅÍ∏àÏ¶ùÏù¥ ÏÉùÍ≤ºÏäµÎãàÎã§.\nEERC721Ïùò Í≤ΩÏö∞ÏóêÎäî token id Î≥ÑÎ°ú 1Í∞úÎßå Í±∞Îûò Í∞ÄÎä•Ìïú Îç∞Ïóê Î∞òÌï¥, ERC1155Îäî Ïó¨Îü¨ Í∞úÎ•º Í±∞ÎûòÌï† Ïàò ÏûàÏäµÎãàÎã§.RC721Ïùò Í≤ΩÏö∞ÏóêÎäî 1Í∞úÎßå Í±∞ÎûòÌïòÎäî Îç∞Ïóê Î∞òÌï¥, ERC1155Îäî Ïó¨Îü¨Í∞úÎ•º Í±∞ÎûòÌï† Ïàò ÏûàÏäµÎãàÎã§.\nÎçîÎ∂àÏñ¥, OpenseaÏùò Í≤ΩÏö∞, Bundle Í±∞ÎûòÍ∞Ä Í∞ÄÎä•Ìï¥ Í∑∏ Í±∞Îûò VolumeÏùÑ Ïñ¥ÎñªÍ≤å Ïû°Îäî ÏßÄ Í∂ÅÍ∏àÌñàÏäµÎãàÎã§.\n\nÍ∑∏ÎûòÏÑú SpellbookÏùÑ Ï°∞ÏÇ¨Ìï¥Î¥§ÏäµÎãàÎã§. [ÎßÅÌÅ¨](https://dune.com/spellbook#!/model/model.spellbook.seaport_v2_ethereum_trades)\n![Screenshot](/notes/dune_nft/images/market_overview/06_spellbook_opensea.png)\n\nÏúÑ Ïä§ÌÅ¨Î¶∞ÏÉ∑ÏóêÏÑú Î≥º Ïàò ÏûàÎìØÏù¥, ÌäπÏ†ï Í±∞ÎûòÏóêÏÑú nftÍ∞Ä Ïó¨Îü¨ Í∞ú Í±∞ÎûòÎêòÎäî Í≤ΩÏö∞ÏóêÎäî bundle Í±∞ÎûòÎ°ú Î∂ÑÎ•òÌïú Îã§Ïùå, Í±∞Îûò ÎåÄÍ∏àÏùÑ Í∑†Îì±ÌïòÍ≤å ÎÇòÎàÑÎäî Í≤ÉÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏóàÏäµÎãàÎã§.\nÏù¥Î•º ÌÜµÌï¥ Îã§ÏùåÍ≥º Í∞ôÏùÄ ÏÇ¨Ïã§ÏùÑ Ïïå Ïàò ÏûàÏäµÎãàÎã§.\n\n- DuneÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî nft trades datasetÏóê Ìè¨Ìï®Îêú opensea Í±∞ÎûòÎäî Ìïú Í±∞ÎûòÏóê Ìè¨Ìï®Îêú nftÎì§Ïùò Í∞úÎ≥Ñ Íµ¨Îß§ ÎåÄÍ∏àÏùÑ Ï†ÑÏ≤¥ ÎåÄÍ∏àÏóêÏÑú Í±∞ÎûòÏóê Ìè¨Ìï®Îêú nftÏùò Í∞úÏàòÎ°ú ÎÇòÎà†ÏÑú Í≥ÑÏÇ∞ÌïúÎã§.\n- Îî∞ÎùºÏÑú, ÏùºÎ∂Ä Í±∞Îûò ÎåÄÍ∏à Îç∞Ïù¥ÌÑ∞Îäî Î∂ÄÏ†ïÌôïÌïòÍ≤å Í≥ÑÏÇ∞ÎêòÏóàÎã§.\n- Îã§Îßå, Ïã§Ï†úÎ°úÎèÑ Bundle Í±∞ÎûòÍ∞Ä ÏùºÏñ¥ÎÇ† Í≤ΩÏö∞ÏóêÎäî ÌåêÎß§Ïûê Ïó≠Ïãú BundleÏóê Ìè¨Ìï®Îêú NFTÎì§Ïùò Í∞úÎ≥Ñ Í∞ÄÍ≤©ÏùÑ Ï∂îÏ†ïÌïú Îã§Ïùå Ìï©ÏÇ∞Ìï¥ÏÑú ÌåêÎß§ÌïòÍ≥†, Ïù¥Îäî Íµ¨Îß§Ïûê ÏûÖÏû•ÏóêÏÑúÎèÑ ÎèôÏùºÌïòÍ≤å Ï†ÅÏö©ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê, Ïã§Ï†ú ÌåêÎß§ÏûêÏôÄ Íµ¨Îß§Ïûê ÏûÖÏû•ÏóêÏÑúÎäî ÌöåÍ≥Ñ Ï≤òÎ¶¨Í∞Ä ÌïÑÏöîÌïú ÏÉÅÌô©Ïù¥ ÏïÑÎãàÎùºÎ©¥ Î≥∏ Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏóê ÌÅ∞ Î¨∏Ï†úÍ∞Ä ÏóÜÎã§.\n\n## Further Study\n\nSpellbookÏùÑ Ï°∞ÏÇ¨ÌïòÎäî Í≥ºÏ†ïÏóêÏÑú Ï¢Ä Îçî Ï°∞ÏÇ¨ÌïòÍ≥† Ïã∂ÏùÄ Í≤ÉÏù¥ ÏÉùÍ≤ºÏäµÎãàÎã§.\nOpenseaÏóêÏÑú Î∞úÏÉùÌïú Bundle Í±∞ÎûòÎäî Ï†ïÌôïÌïú Íµ¨Îß§ ÎåÄÍ∏àÏùÑ Ï∂îÏ†ïÌïòÍ∏∞ Ïñ¥Î†µÎã§Îäî ÏÇ¨Ïã§ÏùÄ ÌôïÏù∏ÌñàÎäîÎç∞,\nGem, BlurÏôÄ Í∞ôÏùÄ Market AggregatorÏóêÏÑú Ïó¨Îü¨ MarketplaceÏùò ListingÏùÑ SweepÌïòÎäî Í±∞ÎûòÏùò Í≤ΩÏö∞Îäî Ïñ¥ÎñªÍ≤å Íµ¨Îß§ ÎåÄÍ∏àÏùÑ ÏÇ∞Ï†ïÌïòÎäî ÏßÄ ÌôïÏù∏Ìï¥Î≥¥Î†§Í≥† Ìï©ÎãàÎã§.\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/private/study/algorithm":{"title":"Algorithm","content":"\n\n## Ïù¥ÏßÑÌÉêÏÉâ Î¨∏Ï†ú\n- Íµ¨Ìï¥Ïïº ÌïòÎäî Í∞íÍ≥º, Í∑∏ Í∞íÏóê Îî∞Îùº Ï†ïÌï¥ÏßÄÎäî Î≥ÄÏàò Í∞íÎì§Ïóê ÎåÄÌïú ÏµúÏ†ÅÌôîÍ∞Ä ÌïÑÏöîÌï† Îïå Î∞©Ï†ïÏãù ÎòêÎäî Î∂ÄÎì±ÏãùÏùò ÌòïÌÉúÎ°ú Ï†ïÏùòÌï† Ïàò ÏûàÎäî Î¨∏Ï†úÎäî Ïù¥ÏßÑ ÌÉêÏÉâÏúºÎ°ú Ìï¥Í≤∞Ìï©ÎãàÎã§.\n- Í∞ÄÏû• ÎåÄÌëúÏ†ÅÏù∏ Î¨∏Ï†úÎäî ÎåÄÍ∏∞Ïó¥Ïóê ÏÑúÏûàÎäî nÎ™ÖÏùò ÏÇ¨ÎûåÎì§Í≥º Ïù¥ ÎåÄÍ∏∞Ïó¥ÏùÑ ÏÜåÌôîÌï† Ïàò ÏûàÎäî Í∞ÅÍ∏∞ Îã§Î•∏ Ï≤òÎ¶¨ ÏãúÍ∞ÑÏùÑ Í∞ÄÏßÑ Ï≤òÎ¶¨Í∏∞ ÏßëÌï©ÏùÑ ÎÜìÍ≥†, Ï≤òÎ¶¨ Í∞ÄÎä•Ìïú ÏµúÏÜå ÏãúÍ∞ÑÏùÑ Íµ¨ÌïòÎäî Î¨∏Ï†úÏûÖÎãàÎã§.\n- Ïù¥ Í≤ΩÏö∞ÏóêÎäî ÏµúÏÜå ÏãúÍ∞ÑÏùÑ Î™©Ï†ÅÍ∞íÏúºÎ°ú ÌïòÍ≥†, Ïù¥ ÏµúÏÜå ÏãúÍ∞ÑÏóê Îî∞Îùº Í∞Å Ï≤òÎ¶¨Í∏∞Í∞Ä Ï≤òÎ¶¨ÌïòÎäî ÏÇ¨ÎûåÏùò ÏàòÎ•º Ïó≠ÏúºÎ°ú Ï∞æÎäî Í≥ºÏ†ïÏóêÏÑú Ìö®Ïú®Ï†ÅÏù∏ Ï≤òÎ¶¨Î•º ÏúÑÌï¥ ÏµúÏÜå ÏãúÍ∞ÑÏù¥ Í∞ÄÏßà Ïàò ÏûàÎäî ÏµúÎåÄ Í∞íÏóêÏÑúÎ∂ÄÌÑ∞ Ïù¥ÏßÑ ÌÉêÏÉâÏúºÎ°ú ÏµúÏÜå ÏãúÍ∞ÑÏùÑ Ï∂îÏ†ïÌïòÎäî Î∞©ÏãùÏúºÎ°ú ÏïåÍ≥†Î¶¨Ï¶òÏùÑ Ìï¥Í≤∞Ìï¥Ïïº Ìï©ÎãàÎã§.\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/private/study/data_engineering":{"title":"Airflow","content":"\n\n## Airflow on Kubernetes Î∞∞Ìè¨ÌïòÍ∏∞ (on M1 Mac)\n- m1Ïóê minikube ÏÑ§ÏπòÌïòÍ∏∞ [Ï∞∏Ï°∞](https://velog.io/@pinion7/macOs-m1-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-kubernetes-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0)\n    ```shell\n    $ curl -Lo minikube https://github.com/kubernetes/minikube/releases/download/v1.25.1/minikube-darwin-arm64 \\\n  \u0026\u0026 chmod +x minikube\n    ```\n    ```shell\n    $ sudo install minikube /usr/local/bin/minikube\n    ```\n- heml, kubectl ÏÑ§ÏπòÌïòÍ∏∞\n    ```shell\n    $ brew install helm, kubectl\n    ```\n- minikube ÏãúÏûë\n    ```shell\n    $ minikube start --driver=docker\n    ```\n- helm repo Ï∂îÍ∞Ä\n    ```shell\n    $ helm repo add apache-airflow https://airflow.apache.org\n    $ helm repo update\n    ```\n- airflow chart ÏÑ§Ïπò\n    ```shell\n    $ helm install $RELEASE_NAME apache-airflow/airflow --namespace $NAMESPACE --debug\n    ```\n- webserver port-forwarding\n    ```shell\n    $ kubectl port-forward svc/airflow-webserver 8080:8080 --namespace $NAMESPACE\n    ```\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/private/study/sql":{"title":"SQL","content":"\n\n## Rollup Query\n- ÏÇ¨Ïö© Î∞©Î≤ï: SUM(COLUMN_2) ... GROUP BY COLUMN_1, ROLLUP(COLUMN_1)\n- Í≤∞Í≥º: Column 1 Î≥Ñ Column 2Ïùò SUM Í≤∞Í≥ºÎì§Ïù¥ ÌëúÌòÑÎêòÎ©¥ÏÑú Í∑∏ Îã§Ïùå RowÏóê SUM Í≤∞Í≥ºÎì§Ïùò Ìï©ÎèÑ ÌëúÌòÑÎêúÎã§.\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null}}