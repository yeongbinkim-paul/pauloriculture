<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Idea for good Data Warehousing (EN) A good data engineer should be able to manage a data warehouse well. Because although many say that the scope of what a data engineer does is too broad, the most important task is building a good place to store and work with data."><meta property="og:title" content="[DE] Idea for good Data Warehousing (EN)"><meta property="og:description" content="Idea for good Data Warehousing (EN) A good data engineer should be able to manage a data warehouse well. Because although many say that the scope of what a data engineer does is too broad, the most important task is building a good place to store and work with data."><meta property="og:type" content="website"><meta property="og:image" content="https://yeongbinkim-paul.github.io/pauloriculture/icon.png"><meta property="og:url" content="https://yeongbinkim-paul.github.io/pauloriculture/notes/data_engineering/idea_for_good_data_warehousing_en/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="[DE] Idea for good Data Warehousing (EN)"><meta name=twitter:description content="Idea for good Data Warehousing (EN) A good data engineer should be able to manage a data warehouse well. Because although many say that the scope of what a data engineer does is too broad, the most important task is building a good place to store and work with data."><meta name=twitter:image content="https://yeongbinkim-paul.github.io/pauloriculture/icon.png"><meta name=twitter:site content="paulkim_bin"><title>[DE] Idea for good Data Warehousing (EN)</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://yeongbinkim-paul.github.io/pauloriculture//icon.png><link href=https://yeongbinkim-paul.github.io/pauloriculture/styles.b369a84b3c6e6bfd686ad1f9da65641c.min.css rel=stylesheet><link href=https://yeongbinkim-paul.github.io/pauloriculture/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://yeongbinkim-paul.github.io/pauloriculture/js/darkmode.696c70eb1d0d27126d8e5ebb1f65d210.min.js></script>
<script src=https://yeongbinkim-paul.github.io/pauloriculture/js/util.a0ccf91e1937fe761a74da4946452710.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script async src=https://yeongbinkim-paul.github.io/pauloriculture/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://yeongbinkim-paul.github.io/pauloriculture/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://yeongbinkim-paul.github.io/pauloriculture/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://yeongbinkim-paul.github.io/pauloriculture/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!0,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://yeongbinkim-paul.github.io/pauloriculture/",fetchData=Promise.all([fetch("https://yeongbinkim-paul.github.io/pauloriculture/indices/linkIndex.e77416d15b21f300430f657f3a41b113.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://yeongbinkim-paul.github.io/pauloriculture/indices/contentIndex.7628b4a697a29ca132d6edf25d902b35.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://yeongbinkim-paul.github.io/pauloriculture",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://yeongbinkim-paul.github.io/pauloriculture",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/yeongbinkim-paul.github.io\/pauloriculture\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=yeongbinkim-paul.github.io/pauloriculture src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script defer type=module src=https://yeongbinkim-paul.github.io/pauloriculture/js/semantic-search.eab1c5cdec473afad2813c1ffdfbd9d0.min.js></script><div class=singlePage><header><h1 id=page-title><a class=root-title href=https://yeongbinkim-paul.github.io/pauloriculture/>ğŸª´ Pauloriculture</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>[DE] Idea for good Data Warehousing (EN)</h1><p class=meta>Last updated
Jul 1, 2023</p><ul class=tags><li><a href=https://yeongbinkim-paul.github.io/pauloriculture/tags/Idea/>Idea</a></li><li><a href=https://yeongbinkim-paul.github.io/pauloriculture/tags/Data-Warehousing/>Data warehousing</a></li><li><a href=https://yeongbinkim-paul.github.io/pauloriculture/tags/Data-Stage/>Data stage</a></li><li><a href=https://yeongbinkim-paul.github.io/pauloriculture/tags/Data-Engineering/>Data engineering</a></li><li><a href=https://yeongbinkim-paul.github.io/pauloriculture/tags/EN/>En</a></li></ul><aside class=mainTOC><details open><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#what-is-data-warehouse>What is Data Warehouse?</a></li><li><a href=#domain-knowledge-for-designing-a-data-warehouse>Domain knowledge for Designing a Data Warehouse</a></li><li><a href=#demand-forecasting-in-the-data-warehouse-design-process>Demand forecasting in the data warehouse design process</a></li><li><a href=#ë‚´ê°€-ë°ì´í„°-ì•„í‚¤í…ì³ë¥¼-ë§Œë“ ë‹¤ë©´>ë‚´ê°€ ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ë§Œë“ ë‹¤ë©´,</a><ol><li><a href=#1-understand-as-much-as-you-can-about-the-data-you-have-right-now>1. Understand as much as you can about the data you have right now.</a></li><li><a href=#2-determine-what-you-must-accomplish-first-and-how-you-will-get-there>2. Determine what you must accomplish first and how you will get there.</a></li><li><a href=#3-plan-your-data-stages>3. Plan your data stages.</a></li><li><a href=#4-plan-how-you-will-manage-your-data-lineage>4. Plan how you will manage your data lineage.</a></li><li><a href=#5-plan-how-you-will-validate-verify-and-monitor-your-data>5. Plan how you will validate, verify, and monitor your data.</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></details></aside><a href=#idea-for-good-data-warehousing-en><h1 id=idea-for-good-data-warehousing-en><span class=hanchor arialabel=Anchor># </span>Idea for good Data Warehousing (EN)</h1></a><p>A good data engineer should be able to manage a data warehouse well.
Because although many say that the scope of what a data engineer does is too broad, the most important task is building a good place to store and work with data.
Today I&rsquo;ve written a few important things to remember while building and managing a data warehouse.</p><a href=#what-is-data-warehouse><h2 id=what-is-data-warehouse><span class=hanchor arialabel=Anchor># </span>What is Data Warehouse?</h2></a><p>Let&rsquo;s start with the definition of a data warehouse.
First introduced in 1992 in the book Building the Data Warehouse by Inmon W.H., a data warehouse is where data is gathered from multiple data sources, subject-oriented, and integrated. Inmon also says that a data warehouse should be time-variant, meaning that all data can be identified in a specific time interval, and non-volatile, meaning that data can be added but not erased to maintain a consistent purpose.</p><p>This data warehouse is defined in its most general scope, so there is a difference between that and the data warehouses that do the job nowadays.
This is because today&rsquo;s data warehouses are complete with many other external factors, including the purpose of the data and the cost of storing and processing it.</p><p>I will talk about some things that can be added to this definition that are important to consider when designing and building a data warehouse.</p><a href=#domain-knowledge-for-designing-a-data-warehouse><h2 id=domain-knowledge-for-designing-a-data-warehouse><span class=hanchor arialabel=Anchor># </span>Domain knowledge for Designing a Data Warehouse</h2></a><p>Domain knowledge is critical because data engineers should work with data based on well-developed logic.
If data engineers have a background in where the data is being used, they can distinguish the correlation and causation of each piece of data. If not, this will not be treating the data based on logic. It will be data processing to make it look good and will not serve any purpose.
That&rsquo;s why as a data engineer, you must have good domain knowledge and try to understand all of the data you&rsquo;re dealing with, in what form it exists, and how it&rsquo;s used.</p><p>While this may seem like a similar perspective to domain-driven design, it&rsquo;s a little different.
It&rsquo;s well known that you can achieve high productivity, low complexity, and easy communication by adopting a domain-driven design in general. A data warehouse with domain knowledge can achieve various performance optimizations that best reflect the current data characteristics while keeping the data as usable as possible.</p><a href=#demand-forecasting-in-the-data-warehouse-design-process><h2 id=demand-forecasting-in-the-data-warehouse-design-process><span class=hanchor arialabel=Anchor># </span>Demand forecasting in the data warehouse design process</h2></a><p>If there is one remaining challenge for data engineers with good domain knowledge, it&rsquo;s demand forecasting.
Demand forecasting refers to forecasting the demand for data that consumers will frequently use in the data warehouse for analytical purposes and preparing that data in a usable form.
Most data is multidimensional, meaning that the same data can be separated into multiple dimensions and used for analysis.
As mentioned earlier, Data engineers must work with data based on well-developed logic.
When dealing with multidimensional data, you must analyze multiple dimensions to see correlations.
However, not all correlations lead to meaningful causal analysis, so data engineers must have many conversations with consumers who use the data they create.
Often, the consumers, in this case, will be people in cross-functional roles, such as data analysts, backend engineers, project managers, and product managers. These people have the same goal: to work with data, so they should have a broad understanding of a well-defined goal.
In creating a data product, you must draw insights from multiple data points. As a data engineer, you work closest to the data sources, so you have the first access to this data and can easily do the most basic analysis.</p><a href=#ë‚´ê°€-ë°ì´í„°-ì•„í‚¤í…ì³ë¥¼-ë§Œë“ ë‹¤ë©´><h2 id=ë‚´ê°€-ë°ì´í„°-ì•„í‚¤í…ì³ë¥¼-ë§Œë“ ë‹¤ë©´><span class=hanchor arialabel=Anchor># </span>ë‚´ê°€ ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ë§Œë“ ë‹¤ë©´,</h2></a><p>ê·¸ë ‡ê¸° ë•Œë¬¸ì—, ì˜ ê°–ì¶°ì§„ ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ë¨¼ì € ë°ì´í„°ì˜ ì „ë°˜ì ì¸ ì´í•´ë¥¼ ê°–ì¶”ê³ , ì†Œë¹„ìì˜ ìˆ˜ìš”ë¥¼ ì´í•´í•œ ë‹¤ìŒì— ì´ì— ë”°ë¼ ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.
ì´ ê³¼ì •ì„ ì„¸ì‹¬í•˜ê²Œ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ë©´, ì—¬ëŸ¬ ê°€ì§€ ë°ì´í„° ìˆ˜ìš”ì— ëŒ€ì‘í•˜ì§€ ëª»í•˜ê³ , ìˆ˜ì‹œë¡œ ë°ì´í„° ìŠ¤í‚¤ë§ˆë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ë°ì´í„° ì•„í‚¤í…ì³ ë¦¬íŒ©í† ë§ì„ í•˜ëŠë¼ ì‹œê°„ì„ ì†Œëª¨í•˜ê²Œ ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.
ì œê°€ ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ì²˜ìŒë¶€í„° ì„¤ê³„í•œë‹¤ë©´, ì–´ë–¤ ê²ƒë“¤ì„ ê³ ë ¤í•˜ê³ ì í•˜ëŠ” ì§€ ì ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.</p><a href=#1-understand-as-much-as-you-can-about-the-data-you-have-right-now><h3 id=1-understand-as-much-as-you-can-about-the-data-you-have-right-now><span class=hanchor arialabel=Anchor># </span>1. Understand as much as you can about the data you have right now.</h3></a><p>Data architectures designed without fully understanding the data on your desk are short-lived. As I said above, you need to take the time to understand the data you have and what you need to accomplish by communicating with the consumers of that data.</p><a href=#2-determine-what-you-must-accomplish-first-and-how-you-will-get-there><h3 id=2-determine-what-you-must-accomplish-first-and-how-you-will-get-there><span class=hanchor arialabel=Anchor># </span>2. Determine what you must accomplish first and how you will get there.</h3></a><p>Every data architecture has a business feasibility and a concise, maintainable architecture that considers all possibilities doesn&rsquo;t exist in the real world. First and foremost, you need to consider how you can accomplish what you need to complete, and in doing so, adopt the easiest and least expensive tech spec and implementation possible because the business logic will inevitably become more complex.</p><p>Here are some of the things I like to consider during this process
a) Requirements from the consumer perspective:
Please communicate with the data analysts and backend engineers who will consume the output of your data pipeline to understand their requirements. There should be an agreement on the information that must be included, the data format, and the frequency of data updates. This could take the form of an SLA. Ideally, if you&rsquo;re looking for a flawless implementation, you should be able to break down achievable goals with business feasibility, calculate a timeline, and get the work done.</p><p>b) Selecting tech specs:
You need to select a tech spec for the implementation, which includes:
1. the data infrastructure architecture,
2. the technical familiarity of the team members with the data infrastructure,
3. a weekly/monthly cost estimate for the data infrastructure, and
4. a time estimate for the implementation.
The problem is that all the factors are highly uncertain, and it&rsquo;s hard to determine the order of importance. This is because the problem you&rsquo;re trying to solve looks different for every team in every company.
I think the first thing to do in this situation is to do a Pre-Mortem with your team.
In his book &lsquo;Thinking, Fast and Slow&rsquo;, Daniel Kahneman refers to the concept of the planning fallacy, stating that many people become overconfident and fail to carry out their plans by creating unrealistic best-case scenarios or ignoring outside perspectives to find examples of similar situations.
To address the optimism that causes it, he suggests Pre-Mortem. Applying this concept to our problem, we can proceed as follows.</p><ol><li>Get together with your team and explain why this tech spec will fail miserably in 1, 3, or 6 months.</li><li>For each reason for failure, identify ways to prevent it in the planning process.</li><li>Briefly explore external examples that address each reason for failure.</li></ol><p>These steps can prevent the team from collectively conforming to a decision and allow knowledgeable people to use their imaginations to identify risks in a desirable direction.
If you go through this process and then go back and select a tech spec that best satisfies the four considerations above, you will have a much more reliable tech spec.</p><a href=#3-plan-your-data-stages><h3 id=3-plan-your-data-stages><span class=hanchor arialabel=Anchor># </span>3. Plan your data stages.</h3></a><p>For each stage, you need to define the purpose of the executed ETL jobs and the resulting data&rsquo;s role.
For example, one way to plan is to divide the stages into DATA LAKE, where data sources are loaded; DATA WAREHOUSE, which contains processed data; and DATA MART, which the consumers will use for actual products or analytics. The clearer the role of the pipe that processes data in each stage, the better it will be to identify improvement points when improving the structure in the future.
Here&rsquo;s the guiding principle</p><p>a) Rules for Storing data:
Each stage should have clear rules for storing data. For example, not all stages need to be non-volatile, but Source must be non-volatile.
b) Rules for categorizing data:
Make sure you have clear rules for categorizing data in each stage. For example, the nature of data in a DATA MART can be data with new information generated from multiple data sources or data that has a clearly defined use outside of the pipeline. Be clear about the rationale for categorizing and storing data in each stage.
c) Rules for processing data:
Make sure you have clear rules for processing data in each stage. For example, a pipeline that processes data in the stage before the DATA MART can enforce logic that guarantees equality. Or, when processing data in the data lake, you can enforce rules that only allow changing data type or column name.
d) Rules for handling exception:
Respond to any exceptions you may find to the above rules based on business feasibility, but make sure that all exceptions are recorded as re-architecting points so that they can be revisited.</p><p>The trickiest part of these rules is the data categorization rules. For example, the tables in DATA MART are used for Products as they are, but you can join multiple mart tables to create a new table for analysis.
You can make the following judgments to deal with these cases.
a) Consider whether other sources can be used to produce data for analytical purposes instead of using DATA MART tables as sources.
B) You should be able to consider whether you can build a new DATA WAREHOUSE with DATA MART tables as its source for analytical purposes.</p><p>Depending on your business situation, I&rsquo;m sure there are other options to consider, but this is where data engineers grow from much experience and study.</p><a href=#4-plan-how-you-will-manage-your-data-lineage><h3 id=4-plan-how-you-will-manage-your-data-lineage><span class=hanchor arialabel=Anchor># </span>4. Plan how you will manage your data lineage.</h3></a><p>The more complex your business logic becomes, the more likely you are to have a single point of failure due to the people who planned the data architecture in the first place, so you need to think about how to manage Data Lineage in a way that anyone can learn. If it&rsquo;s difficult for everyone to learn, you will end up with SPF.
To ensure that to configure Data Lineage in a way that doesn&rsquo;t hurt your productivity, we recommend avoiding the following situations as much as possible.
a) If you must create a cyclic structure within one data warehouse.
b) When processing data by referring to sources contained in different data stages
Both situations would undermine the definition of the data stages we planned in #3.
These problems are best solved by leveraging domain knowledge, business goals, and stakeholder communication because complex data lineage will eventually become a bottleneck for data productivity.</p><p>There are choices here that you should avoid at all costs.
a) The solution of changing how data analysts or data backend engineers handle things: This solution will eventually lead to you facing the same problem again.
b) Choosing complexity because it is unavoidable and utilizing the resulting data as a source for subsequent data: If you choose to allow exceptions because complexity is an inevitable feature of your structure, you must control that the exceptions allowed do not increase complexity downstream.</p><a href=#5-plan-how-you-will-validate-verify-and-monitor-your-data><h3 id=5-plan-how-you-will-validate-verify-and-monitor-your-data><span class=hanchor arialabel=Anchor># </span>5. Plan how you will validate, verify, and monitor your data.</h3></a><p>We will always encounter unexpected exception data, even with good data architecture.
These exceptions can happen by complex logic that exceeds the scope of allocated resources, outlier inputs that don&rsquo;t have a business impact, or schema changes or version upgrades to the data source.
Data quality is more important to data engineers than anything else, so they must be the first to identify and respond to possible exceptions for all data-handling pipelines.
So, to complete a good data architecture, you need to decide how to organize your validation and verification and how to monitor it.
Here&rsquo;s what I consider important about Validation, Verification, and Monitoring.
a) You should be able to verify that it follows the definition of a Stage.
b) Depending on the data lineage, you should ensure each validation logic that verifies data for a specific point. You should not perform validation of the same objective at multiple stages.
c) You should separately record exception data identified during the validation process and leverage it in the verification rule.
d) You should monitor by identifying the points where the most exceptions occurred during development but should evolve through issues during production to avoid gray areas.</p><a href=#conclusion><h2 id=conclusion><span class=hanchor arialabel=Anchor># </span>Conclusion</h2></a><p>There isn&rsquo;t a right answer to good data architecture because better technologies are always introduced into the world, and more creative solutions can be found. While creating that answer, we should always be careful about what we must catch. What we achieve may not be the best, it may be the second best, but we can create another best if we aim for best practices and don&rsquo;t forget what we need to consider to do that.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script async src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://yeongbinkim-paul.github.io/pauloriculture/js/graph.6579af7b10c818dbd2ca038702db0224.js></script></div></div><div id=contact_buttons><footer><p>Made by Yeongbin Kim (Paul) using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2023</p><ul><li><a href=https://yeongbinkim-paul.github.io/pauloriculture/>Home</a></li><li><a href=https://twitter.com/paulkim_bin>Twitter</a></li><li><a href=https://github.com/yeongbinkim-paul>GitHub</a></li></ul></footer></div></div></body></html>