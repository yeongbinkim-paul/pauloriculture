{"/":{"title":"ğŸª´ Pauloriculture","content":"\n## Nice to meet you.\n## I'm Yeongbin (Paul). I'm a data engineer.\n\u003cimg src=\"pfp.png\" width=\"350\"\u003e\n\n## Welcome to My Brain GardenğŸª´\n\n*â€œThe data product will set me freeâ€*\n\nI am a Driven Data Engineer with 2+ years of experience in crafting data pipelines and backend systems.\n\nProven ability to write clear, well-documented code, enhancing maintainability and troubleshooting efficiency.\n\nExhibited proficiency in independent problem-solving using data-driven approaches.\n\nLeveraged critical thinking and communication skills for efficient cross-functional, interpersonal collaboration.\n\nAlways Ambitious to grow professionally alongside esteemed colleagues and the company with a given mission.\n\nLanguages : English | Korean\n\n## Navigation...\nThere are multiple ways to navigate my Brain Garden:\n\n1. Use the search bar on the top right or press `cmd+k` (`ctrl+k` on Windows) or click on the Search button (top right) to search for any term.\n2. Click on a note to explore its content, and follow the links and backlinks to dive deeper into related topics.\n3. Interact with the graph at the bottom of the page to visualize connections between notes and click on any node to navigate directly to that note.\n4. Click on the [Hashtags](tags) to explore the topics by tags.\n\n## Where can you find me?\n\n**Current Location** : Toronto, Ontario\n\n**My Personal Email** : kimyb132@gmail.com\n\n**Links** : [LinkedIn](https://www.linkedin.com/in/yeongbin-kim-972635118/) | [Github](https://github.com/yeongbinkim-paul)\n\n\n\n| Main Tags          |\n|---------------|\n| [Ideas](tags/idea),[Data Engineering](tags/data-engineering), [SQL](tags/sql), [Pandas](tags/pandas), [EN](tags/EN)  |\n\n\u003c!--\n### Data Engineering Idea Notes (EN)\n1. [Idea for good Data Warehousing (EN)](notes/data_engineering/idea_for_good_data_warehousing_en.md)\n\n### Data Engineering Study Notes (EN)\n1. [Pandas as and SQL (EN)](notes/data_engineering/pandas_as_an_sql_en.md)\n\n### Data Analysis Study Notes (EN)\n1. [Dune Analytics : NFT Market Overview (EN)](notes/dune_nft/market_overview_en.md)\n\n### Data Engineering Idea Notes (KR)\n1. [Idea for good Data Warehousing (KR)](notes/data_engineering/idea_for_good_data_warehousing_kr.md)\n\n### Data Engineering Study Notes (KR)\n1. [Pandas as and SQL (KR)](notes/data_engineering/pandas_as_an_sql_kr.md)\n\n### Data Analysis Study Notes (KR)\n1. [Dune Analytics : NFT Market Overview (KR)](notes/dune_nft/market_overview_kr.md) --\u003e\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/idea_for_good_data_warehousing_en":{"title":"[DE] Idea for good Data Warehousing (EN)","content":"# Go to Korean Edition [Idea for good Data Warehousing (KR)](notes/data_engineering/idea_for_good_data_warehousing_kr.md)\n\n# Idea for good Data Warehousing (EN)\nA good data engineer should be able to manage a data warehouse well.\nBecause although many say that the scope of what a data engineer does is too broad, the most important task is building a good place to store and work with data.\nToday I've written a few important things to remember while building and managing a data warehouse.\n\n## What is Data Warehouse?\nLet's start with the definition of a data warehouse.\n\nFirst introduced in 1992 in the book Building the Data Warehouse by Inmon W.H., a data warehouse is where data is gathered from multiple data sources, subject-oriented, and integrated. Inmon also says that a data warehouse should be time-variant, meaning that all data can be identified in a specific time interval, and non-volatile, meaning that data can be added but not erased to maintain a consistent purpose.\n\nThis data warehouse is defined in its most general scope, so there is a difference between that and the data warehouses that do the job nowadays.\nThis is because today's data warehouses are complete with many other external factors, including the purpose of the data and the cost of storing and processing it.\n\nI will talk about some things that can be added to this definition that are important to consider when designing and building a data warehouse.\n\n## Domain knowledge for Designing a Data Warehouse\nDomain knowledge is critical because data engineers should work with data based on well-developed logic.\nIf data engineers have a background in where the data is being used, they can distinguish the correlation and causation of each piece of data. If not, this will not be treating the data based on logic. It will be data processing to make it look good and will not serve any purpose.\nThat's why as a data engineer, you must have good domain knowledge and try to understand all of the data you're dealing with, in what form it exists, and how it's used.\n\nWhile this may seem like a similar perspective to domain-driven design, it's a little different.\nIt's well known that you can achieve high productivity, low complexity, and easy communication by adopting a domain-driven design in general. A data warehouse with domain knowledge can achieve various performance optimizations that best reflect the current data characteristics while keeping the data as usable as possible.\n\n\n## Demand forecasting in the data warehouse design process\nIf there is one remaining challenge for data engineers with good domain knowledge, it's demand forecasting.\nDemand forecasting refers to forecasting the demand for data that consumers will frequently use in the data warehouse for analytical purposes and preparing that data in a usable form.\n\nIn creating a data product, you must draw insights from multiple data points. As a data engineer, you work closest to the data sources, so you have the first access to this data and can easily do the most basic analysis.\nMost data is multidimensional, meaning that the same data can be separated into multiple dimensions and used for analysis. When dealing with multidimensional data, you must analyze multiple dimensions to see correlations.\nHowever, not all correlations lead to meaningful causal analysis, so data engineers must have many conversations with consumers who use the data they create.\n\n*Often, the consumers, in this case, will be people in cross-functional roles, such as data analysts, backend engineers, project managers, and product managers. These people have the same goal: to work with data, so they should have a broad understanding of a well-defined goal.*\n\nThen you should get ideas of how your data pipeline stores data and how it should be updated.\n\n## If I create a data warehouse architecture,\nAs described above, we believe that a data engineer should have a well-rounded understanding of the data first and foremost, based on well-developed domain knowledge, so that they can understand the consumer's needs and then design the data warehouse architecture accordingly.\nFollow this process carefully to save time refactoring your data warehouse architecture to respond to different data demands and changing your data schema occasionally.\n\nSo, if I were designing a data warehouse architecture from scratch, here's what I would consider.\n\n### 1. Understand as much as you can about the data you have right now.\nData architectures designed without fully understanding the data on your desk are short-lived. As I said above, you need to take the time to understand the data you have and what you need to accomplish by communicating with the consumers of that data.\n\n### 2. Determine what you must accomplish first and how you will get there.\nEvery data architecture has a business feasibility and a concise, maintainable architecture that considers all possibilities doesn't exist in the real world. First and foremost, you need to consider how you can accomplish what you need to complete, and in doing so, adopt the easiest and least expensive tech spec and implementation possible because the business logic will inevitably become more complex.\n\nHere are some of the things I like to consider during this process\n\n#### Requirements from the consumer perspective:\n\nPlease communicate with the data analysts and backend engineers who will consume the output of your data pipeline to understand their requirements. There should be an agreement on the information that must be included, the data format, and the frequency of data updates. This could take the form of an SLA. Ideally, if you're looking for a flawless implementation, you should be able to break down achievable goals with business feasibility, calculate a timeline, and get the work done.\n\n#### Selecting tech specs:\n\nYou need to select a tech spec for the implementation, which includes:\n\n1. the data infrastructure architecture\n\n2. the technical familiarity of the team members with the data infrastructure\n\n3. a weekly/monthly cost estimate for the data infrastructure\n\n4. a time estimate for the implementation.\n\nThe problem is that all the factors are highly uncertain, and it's hard to determine the order of importance. This is because the problem you're trying to solve looks different for every team in every company.\n\nI think the first thing to do in this situation is to do a Pre-Mortem with your team.\nIn his book 'Thinking, Fast and Slow', Daniel Kahneman refers to the concept of the planning fallacy, stating that many people become overconfident and fail to carry out their plans by creating unrealistic best-case scenarios or ignoring outside perspectives to find examples of similar situations.\nTo address the optimism that causes it, he suggests Pre-Mortem. Applying this concept to our problem, we can proceed as follows.\n\n1. Get together with your team and explain why this tech spec will fail miserably in 1, 3, or 6 months.\n\n2. For each reason for failure, identify ways to prevent it in the planning process.\n\n3. Briefly explore external examples that address each reason for failure.\n\nThese steps can prevent the team from collectively conforming to a decision and allow knowledgeable people to use their imaginations to identify risks in a desirable direction.\nIf you go through this process and then go back and select a tech spec that best satisfies the four considerations above, you will have a much more reliable tech spec.\n\n### 3. Plan your data stages.\nFor each stage, you need to define the purpose of the executed ETL jobs and the resulting data's role.\n\nFor example, one way to plan is to divide the stages into DATA LAKE, where data sources are loaded; DATA WAREHOUSE, which contains processed data; and DATA MART, which the consumers will use for actual products or analytics. The clearer the role of the pipe that processes data in each stage, the better it will be to identify improvement points when improving the structure in the future.\n\nHere's the guiding principle\n\n1. Rules for Storing data:\n\n    Each stage should have clear rules for storing data. For example, not all stages need to be non-volatile, but Source must be non-volatile.\n\n2. Rules for categorizing data:\n\n    Make sure you have clear rules for categorizing data in each stage. For example, the nature of data in a DATA MART can be data with new information generated from multiple data sources or data that has a clearly defined use outside of the pipeline. Be clear about the rationale for categorizing and storing data in each stage.\n\n3. Rules for processing data:\n\n    Make sure you have clear rules for processing data in each stage. For example, a pipeline that processes data in the stage before the DATA MART can enforce logic that guarantees equality. Or, when processing data in the data lake, you can enforce rules that only allow changing data type or column name.\n\n4. Rules for handling exception:\n\n    Respond to any exceptions you may find to the above rules based on business feasibility, but make sure that all exceptions are recorded as re-architecting points so that they can be revisited.\n\nThe trickiest part of these rules is the data categorization rules. For example, the tables in DATA MART are used for Products as they are, but you can join multiple mart tables to create a new table for analysis.\n\nYou can make the following judgments to deal with these cases.\n\n1. Consider whether other sources can be used to produce data for analytical purposes instead of using DATA MART tables as sources.\n\n2. You should be able to consider whether you can build a new DATA WAREHOUSE with DATA MART tables as its source for analytical purposes.\n\nDepending on your business situation, I'm sure there are other options to consider, but this is where data engineers grow from much experience and study.\n\n### 4. Plan how you will manage your data lineage.\nThe more complex your business logic becomes, the more likely you are to have a single point of failure due to the people who planned the data architecture in the first place, so you need to think about how to manage Data Lineage in a way that anyone can learn. If it's difficult for everyone to learn, you will end up with SPF.\n\nTo ensure that to configure Data Lineage in a way that doesn't hurt your productivity, we recommend avoiding the following situations as much as possible.\n\n1. If you must create a cyclic structure within one data warehouse.\n\n2. When processing data by referring to sources contained in different data stages\n\nBoth situations would undermine the definition of the data stages we planned in #3.\nThese problems are best solved by leveraging domain knowledge, business goals, and stakeholder communication because complex data lineage will eventually become a bottleneck for data productivity.\n\nThere are choices here that you should avoid at all costs.\n\n1. The solution of changing how data analysts or data backend engineers handle things:\n\n    This solution will eventually lead to you facing the same problem again.\n\n2. Choosing complexity because it is unavoidable and utilizing the resulting data as a source for subsequent data:\n\n    If you choose to allow exceptions because complexity is an inevitable feature of your structure, you must control that the exceptions allowed do not increase complexity downstream.\n\n### 5. Plan how you will validate, verify, and monitor your data.\nWe will always encounter unexpected exception data, even with good data architecture.\nThese exceptions can happen by complex logic that exceeds the scope of allocated resources, outlier inputs that don't have a business impact, or schema changes or version upgrades to the data source.\n\nData quality is more important to data engineers than anything else, so they must be the first to identify and respond to possible exceptions for all data-handling pipelines.\nSo, to complete a good data architecture, you need to decide how to organize your validation and verification and how to monitor it.\n\nHere's what I consider important about Validation, Verification, and Monitoring.\n\n1. You should be able to verify that it follows the definition of a Stage.\n\n2. Depending on the data lineage, you should ensure each validation logic that verifies data for a specific point. You should not perform validation of the same objective at multiple stages.\n\n3.  You should separately record exception data identified during the validation process and leverage it in the verification rule.\n\n4. You should monitor by identifying the points where the most exceptions occurred during development but should evolve through issues during production to avoid gray areas.\n\n## Conclusion\nThere isn't a right answer to good data architecture because better technologies are always introduced into the world, and more creative solutions can be found. While creating that answer, we should always be careful about what we must catch. What we achieve may not be the best, it may be the second best, but we can create another best if we aim for best practices and don't forget what we need to consider to do that.\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/idea_for_good_data_warehousing_kr":{"title":"[DE] Idea for good Data Warehousing (KR)","content":"# Go to English Edition [Idea for good Data Warehousing (EN)](notes/data_engineering/idea_for_good_data_warehousing_en.md)\n\n# Idea for good Data Warehousing (KR)\nì €ëŠ” ì¢‹ì€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ëŠ” ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ ì˜ ê´€ë¦¬í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\nì™œëƒí•˜ë©´, ë¹„ë¡ ë§ì€ ì‚¬ëŒë“¤ì´ ë°ì´í„° ì—”ì§€ë‹ˆì–´ê°€ í•´ì•¼ í•˜ëŠ” ì˜ì—­ì´ ë„ˆë¬´ ë„“ë‹¤ê³  ë§í•˜ì§€ë§Œ, ê·¸ ì¤‘ì—ì„œë„ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ë‹¤ë£¨ëŠ” ê³µê°„ì„ ì˜ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•œ ì„ë¬´ë¼ê³  ìƒê°í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nì˜¤ëŠ˜ì€ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ê´€ë¦¬í•˜ë©´ì„œ ê¼­ ì‹ ê²½ì¨ì•¼ í•œë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒë“¤ì— ëŒ€í•´ì„œ ëª‡ ê°€ì§€ ì ì–´ë³´ì•˜ìŠµë‹ˆë‹¤.\n\n## ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ì •ì˜\nê°€ì¥ ë¨¼ì € ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ì •ì˜ë¶€í„° ì‚´í´ë´…ë‹ˆë‹¤.\n\n1992ë…„ Inmon W.Hì˜ ì±… (Building the Data Warehouse)ì—ì„œ ì²˜ìŒ ì†Œê°œëœ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë€ íšŒì‚¬ì˜ ìš´ì˜ ë°©í–¥ ëŒ€ì‹  íŠ¹ì • ì£¼ì œì— ë§ëŠ” ë°ì´í„°ë¥¼ ì§€ë‹ˆê³  ìˆìœ¼ë©° (Subject Oriented), ì—¬ëŸ¬ ê°€ì§€ ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ìˆ˜ì§‘ëœ ë°ì´í„°ë“¤ì´ ëª¨ì´ëŠ” ê³³ì…ë‹ˆë‹¤ (Integrated). Inmonì€ ë˜í•œ, ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” ëª¨ë“  ë°ì´í„°ê°€ ê°ê° íŠ¹ì • ì‹œê°„ êµ¬ê°„ì—ì„œ ì‹ë³„ë  ìˆ˜ ìˆì–´ì•¼ í•˜ë©° (Time-variant), ë°ì´í„°ê°€ ì¶”ê°€ë  ìˆ˜ ìˆìœ¼ë©´ì„œ ì§€ì›Œì§€ì§€ ì•Šì•„ ì¼ê´€ëœ ëª©ì ì„ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ë¹„íœ˜ë°œì„± (Non-volatile)ì„ ê°–ì¶°ì•¼ í•œë‹¤ê³  ë§í•©ë‹ˆë‹¤.\n\nì €ëŠ” ì´ê²ƒì´ ê°€ì¥ ì¼ë°˜ì ì¸ ë²”ìœ„ì—ì„œ ì •ì˜ëœ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì´ê¸° ë•Œë¬¸ì— í˜„ ì‹œëŒ€ì— ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë“¤ê³¼ëŠ” ë¶„ëª…íˆ ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\ní˜„ì¬ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ëŠ” ë°ì´í„°ì˜ ëª©ì ê³¼, ì €ì¥ ë° ì²˜ë¦¬ ë¹„ìš©ì„ í¬í•¨í•˜ëŠ” ë‹¤ë¥¸ ë‹¤ì–‘í•œ ì™¸ë¶€ ìš”ì¸ë“¤ê¹Œì§€ í¬í•¨í•˜ì—¬ ì™„ì„±ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nì €ëŠ” ì´ ì •ì˜ì— ë§ë¶™ì¼ ìˆ˜ ìˆëŠ”, ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ ì„¤ê³„í•  ë•Œ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ê²ƒë“¤ì„ ëª‡ ê°€ì§€ ì´ì•¼ê¸°í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n\n## ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„¤ê³„ë¥¼ ìœ„í•œ ë„ë©”ì¸ ì§€ì‹\nì €ëŠ” ë°ì´í„° ì—”ì§€ë‹ˆì–´ê°€ ì˜ ê°–ì¶°ì§„ ë…¼ë¦¬ì— ê·¼ê±°í•´ ë°ì´í„°ë¥¼ ë‹¤ë¤„ì•¼ í•œë‹¤ê³  ìƒê°í•˜ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ ì§€ì‹ì€ êµ‰ì¥íˆ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\në°ì´í„° ì—”ì§€ë‹ˆì–´ê°€ ë°ì´í„°ê°€ ì“°ì´ëŠ” ê³³ì— ëŒ€í•œ ë°°ê²½ì§€ì‹ì´ ì—†ë‹¤ë©´, ê°ê°ì˜ ë°ì´í„°ê°€ ê°€ì§„ ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ ì œëŒ€ë¡œ êµ¬ë¶„í•˜ì§€ ëª»í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ë…¼ë¦¬ì— ê·¼ê±°í•´ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë³´ê¸° ì¢‹ê²Œ ê¾¸ë©°ì§„ ë°ì´í„° ê°€ê³µì´ ë˜ì–´ ì–´ë–¤ íš¨ìš©ë„ ì—†ê²Œ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.\nì €ëŠ” ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„° ì—”ì§€ë‹ˆì–´ë¼ë©´ ë„ë©”ì¸ ì§€ì‹ì„ ì˜ ê°–ì¶”ê³ , ë‹¤ë¤„ì•¼ í•  ë°ì´í„°ë“¤ì´ ì–´ë–¤ í˜•íƒœë¡œ ì¡´ì¬í•˜ëŠ” ì§€, ê·¸ë¦¬ê³  ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì“°ì´ëŠ” ì§€ê¹Œì§€ ì „ë¶€ ì´í•´í•˜ë ¤ê³  ë…¸ë ¥í•´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n\në„ë©”ì¸ ì£¼ë„ ì„¤ê³„ì™€ ë¹„ìŠ·í•œ ê´€ì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ì¡°ê¸ˆì€ ë‹¤ë¥¸ ì ì´ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\në„ë©”ì¸ ì£¼ë„ ì„¤ê³„ë¥¼ ì±„íƒí•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ìƒì‚°ì„±ê³¼ ë‚®ì€ ë³µì¡ë„, ì‰¬ìš´ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ê³  ì˜ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ë„ë©”ì¸ ì§€ì‹ì„ ê°–ì¶˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” ë°ì´í„°ì˜ í™œìš© ë²”ìœ„ë¥¼ ê°€ëŠ¥í•œ í•œ ìµœëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ ìˆìœ¼ë©´ì„œ, í˜„ì¬ ë°ì´í„° íŠ¹ì„±ì„ ê°€ì¥ ì˜ ë°˜ì˜í•˜ë©´ì„œ ë‹¤ì–‘í•œ ì„±ëŠ¥ ìµœì í™”ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n## ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„¤ê³„ ê³¼ì •ì—ì„œì˜ ìˆ˜ìš” ì˜ˆì¸¡\në„ë©”ì¸ ì§€ì‹ì„ ì˜ ì´í•´í•œ ë°ì´í„° ì—”ì§€ë‹ˆì–´ì—ê²Œ ë˜ í•˜ë‚˜ ë‚¨ì€ ê³¼ì œê°€ ìˆë‹¤ë©´, ìˆ˜ìš” ì˜ˆì¸¡ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\nìˆ˜ìš” ì˜ˆì¸¡ì€ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œ ì†Œë¹„ìë“¤ì´ ë¶„ì„ ëª©ì ìœ¼ë¡œ ìì£¼ ì‚¬ìš©í•  ë°ì´í„°ì˜ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ê·¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ì¤€ë¹„í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\në°ì´í„° í”„ë¡œë•íŠ¸ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì—ì„œëŠ” ì—¬ëŸ¬ ê°€ì§€ ë°ì´í„° í¬ì¸íŠ¸ë¡œë¶€í„° ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„° ì—”ì§€ë‹ˆì–´ëŠ” ê°€ì¥ ë°ì´í„° ì†ŒìŠ¤ì™€ ë°€ì ‘í•œ ìœ„ì¹˜ì—ì„œ ì¼í•˜ëŠ” ì‚¬ëŒì´ê¸° ë•Œë¬¸ì—, ì´ ë°ì´í„°ë“¤ì— ê°€ì¥ ë¨¼ì € ì ‘ê·¼í•  ìˆ˜ ìˆê³ , ê°€ì¥ ê¸°ë³¸ì ì¸ ë¶„ì„ì„ ì‰½ê²Œ í•´ë‚¼ ìˆ˜ ìˆëŠ” ì—­í• ì— ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.\nëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ëŠ” ë‹¤ì°¨ì› ë°ì´í„° í˜•íƒœë¥¼ ê°–ê³  ìˆìŠµë‹ˆë‹¤. ê°™ì€ ë°ì´í„°ë¼ë„ ì—¬ëŸ¬ ì°¨ì›ìœ¼ë¡œ ë¶„ë¦¬ë˜ì–´ ë¶„ì„ì— ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\në‹¤ì°¨ì› ë°ì´í„°ë¥¼ ë‹¤ë£¬ë‹¤ë©´, ê·¸ ì†ì—ì„œ ì—¬ëŸ¬ ì°¨ì›ì— ëŒ€í•œ ë¶„ì„ì„ í†µí•´ ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\nê·¸ë ‡ì§€ë§Œ ëª¨ë“  ìƒê´€ê´€ê³„ê°€ ìœ ì˜ë¯¸í•œ ì¸ê³¼ê´€ê³„ ë¶„ì„ìœ¼ë¡œ ì´ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„° ì—”ì§€ë‹ˆì–´ëŠ” ìì‹ ì´ ë§Œë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì†Œë¹„ìì™€ ë§ì€ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n*ëŒ€ê°œ ì´ ê²½ìš°ì— ì†Œë¹„ìëŠ” ë°ì´í„° ë¶„ì„ê°€, ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´, í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €, í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €ì²˜ëŸ¼ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ì‚¬ëŒë“¤ì¼ ê²ƒì…ë‹ˆë‹¤. ê²°êµ­ ì´ ëª¨ë“  êµ¬ì„±ì›ì€ ê°™ì€ ëª©ì ì„ ê°–ê³  ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì‚¬ëŒë“¤ì´ë¼ëŠ” ê³µí†µì ì´ ìˆê¸° ë•Œë¬¸ì—, ì˜ ì„¤ì •ëœ ëª©í‘œë¥¼ ë„“ê²Œ ì´í•´í•œ ìƒíƒœì—¬ì•¼ í•˜ê² ìŠµë‹ˆë‹¤.*\n\nì´ë¥¼ í†µí•´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ì–´ë–¤ ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , ì—…ë°ì´íŠ¸ ë˜ì–´ì•¼ í•˜ëŠ” ì§€ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n## ë‚´ê°€ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì³ë¥¼ ë§Œë“ ë‹¤ë©´,\nìœ„ì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ë°ì´í„° ì—”ì§€ë‹ˆì–´ëŠ” ì˜ ê°–ì¶°ì§„ ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ë¨¼ì € ë°ì´í„°ì˜ ì „ë°˜ì ì¸ ì´í•´ë¥¼ ê°–ì¶”ê³ , ì†Œë¹„ìì˜ ìˆ˜ìš”ë¥¼ ì´í•´í•œ ë‹¤ìŒì— ì´ì— ë”°ë¼ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì³ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\nì´ ê³¼ì •ì„ ì„¸ì‹¬í•˜ê²Œ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤ë©´, ë‹¹ì‹ ì€ ì—¬ëŸ¬ ê°€ì§€ ë°ì´í„° ìˆ˜ìš”ì— ëŒ€ì‘í•˜ê±°ë‚˜, ìˆ˜ì‹œë¡œ ë°ì´í„° ìŠ¤í‚¤ë§ˆë¥¼ ë³€ê²½í•˜ëŠ” ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” ë“±ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì³ ë¦¬íŒ©í† ë§ì„ í•˜ëŠë¼ ê·€í•œ ì‹œê°„ì„ ì†Œëª¨í•˜ê²Œ ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\n\nì´ì— ë”°ë¼, ë§Œì¼ ì œê°€ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì³ë¥¼ ì²˜ìŒë¶€í„° ì„¤ê³„í•œë‹¤ë©´, ì–´ë–¤ ê²ƒë“¤ì„ ê³ ë ¤í•˜ê³ ì í•˜ëŠ” ì§€ ì•„ë˜ì— ì ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\n\n### 1. ê°€ëŠ¥í•œ í•œ ì§€ê¸ˆ ì£¼ì–´ì§„ ë°ì´í„°ë“¤ì— ëŒ€í•´ ìµœëŒ€í•œ ì´í•´í•©ë‹ˆë‹¤.\nì±…ìƒ ìœ„ì— í¼ì³ì§„ ë°ì´í„°ë“¤ì„ ì¶©ë¶„íˆ ì´í•´í•˜ì§€ ëª»í•œ ìƒíƒœì—ì„œ ì„¤ê³„í•œ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì•„í‚¤í…ì³ëŠ” ìˆ˜ëª…ì´ ì§§ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ìœ„ì—ì„œ ë§í•œ ê²ƒ ì²˜ëŸ¼ ë³´ìœ í•œ ë°ì´í„°ë¥¼ ì´í•´í•˜ê³ , ì´ ë°ì´í„°ì˜ ì†Œë¹„ìë“¤ê³¼ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ í†µí•´ ë‹¬ì„±í•´ì•¼ í•˜ëŠ” ëª©í‘œë¥¼ ì´í•´í•  ì‹œê°„ì´ ê¼­ í•„ìš”í•©ë‹ˆë‹¤.\n\n### 2. ê°€ì¥ ë¨¼ì € ë‹¬ì„±í•´ì•¼ í•˜ëŠ” ëª©í‘œì™€ ê·¸ ëª©í‘œë¥¼ ë‹¬ì„±í•  ë°©ë²•ì„ í™•ì¸í•©ë‹ˆë‹¤.\nì €ëŠ” ëª¨ë“  ë°ì´í„° ì•„í‚¤í…ì³ì—ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ë‹¹ì„±ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ëª¨ë“  ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•œ ê°„ê²°í•˜ê³  ìœ ì§€ë³´ìˆ˜í•˜ê¸° ì¢‹ì€ ì•„í‚¤í…ì³ëŠ” í˜„ì‹¤ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € ë‹¬ì„±í•´ì•¼ í•˜ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ê³¼ì •ì—ì„œ ìµœëŒ€í•œ ì‰½ê³  ì ì€ ë¹„ìš©ì˜ í…Œí¬ ìŠ¤í™ê³¼ êµ¬í˜„ ë°©ì‹ì„ ì±„íƒí•´ì•¼ í•©ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì€ ë‹¹ì—°í•˜ê²Œë„ ë” ë³µì¡í•´ì§ˆ ê²ƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nì´ ê³¼ì •ì—ì„œ ì œê°€ ê³ ë ¤í•˜ê³ ì í•˜ëŠ” ê²ƒë“¤ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n#### ì†Œë¹„ì ê´€ì ì—ì„œì˜ ìš”êµ¬ì‚¬í•­:\në°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ê²°ê³¼ë¬¼ì„ ì†Œë¹„í•  Data Analyst, Backend Engineerë“¤ê³¼ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ í†µí•´ ìš”êµ¬ì‚¬í•­ì„ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤. í•„ìˆ˜ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ” ì •ë³´ì™€, ë°ì´í„° í˜•ì‹, ë°ì´í„° ì—…ë°ì´íŠ¸ ì£¼ê¸°ì— ëŒ€í•´ì„œ í•©ì˜ê°€ ì´ë¤„ì ¸ì•¼ í•©ë‹ˆë‹¤. ê·¸ê²ƒì€ SLAì˜ í˜•íƒœì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì´ìƒì ìœ¼ë¡œ ì™„ë²½í•œ êµ¬í˜„ì´ ìš”êµ¬ëœë‹¤ë©´, ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ë‹¹ì„±ì„ ê³ ë ¤í•´ ë‹¬ì„± ê°€ëŠ¥í•œ ëª©í‘œë¥¼ ì„¸ë¶„í™”í•´ì„œ ì¼ì •ì„ ì‚°ì¶œí•˜ê³  ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n#### í…Œí¬ ìŠ¤í™ ì„ ì •:\nêµ¬í˜„ì„ ìœ„í•´ì„œ í…Œí¬ ìŠ¤í™ì„ ì„ ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ë•Œ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ê²ƒì€,\n\n1. ë°ì´í„° ì¸í”„ë¼ ì•„í‚¤í…ì³\n\n2. ë°ì´í„° ì¸í”„ë¼ì— ëŒ€í•œ íŒ€ì›ë“¤ì˜ ê¸°ìˆ  ì¹œí™”ë„\n\n3. ë°ì´í„° ì¸í”„ë¼ì— ëŒ€í•œ ì£¼/ì›” ë‹¨ìœ„ ë¹„ìš© ì˜ˆì¸¡\n\n4. êµ¬í˜„ì— í•„ìš”í•œ ì‹œê°„ ì˜ˆì¸¡ì´ ìˆìŠµë‹ˆë‹¤.\n\në“±ì´ ìˆìŠµë‹ˆë‹¤.\n\nëª¨ë“  ìš”ì†Œê°€ ë¶ˆí™•ì‹¤ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ì¤‘ìš”ë„ì˜ ìˆœì„œë¥¼ ê²°ì •í•˜ê¸° ì–´ë µë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. í’€ê³ ì í•˜ëŠ” ë¬¸ì œê°€ ëª¨ë“  íšŒì‚¬ì˜ ëª¨ë“  íŒ€ì—ì„œ ê°ê¸° ë‹¤ë¥¸ ëª¨ìŠµì„ í•˜ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nì €ëŠ” ì´ëŸ° ìƒí™©ì—ì„œëŠ” ê°€ì¥ ë¨¼ì € íŒ€ì›ë“¤ê³¼ í•¨ê»˜, Pre-Mortemì„ ì§„í–‰í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ ë  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\nëŒ€ë‹ˆì–¼ ì¹´ë„ˆë¨¼ì€ ì±… 'ìƒê°ì— ê´€í•œ ìƒê°'ì—ì„œ ê³„íš ì˜¤ë¥˜ë¼ëŠ” ê°œë…ì„ ì–¸ê¸‰í•˜ë©°, ë§ì€ ì‚¬ëŒë“¤ì´ ê³¼ì‹ ì— ë¹ ì§„ ë‚˜ë¨¸ì§€ ê³„íšì„ ì„¸ìš¸ ë•Œ, ë¹„í˜„ì‹¤ì ìœ¼ë¡œ ìµœìƒì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì§œê±°ë‚˜, ë¹„ìŠ·í•œ ë‹¤ë¥¸ ìƒí™©ì˜ ì‚¬ë¡€ë¥¼ ì°¾ì•„ë³´ëŠ” ì™¸ë¶€ ê´€ì ì„ ë¬´ì‹œí•´ ê³„íšì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì— ì‹¤íŒ¨í•œë‹¤ê³  ë§í•©ë‹ˆë‹¤.\nì €ëŠ” ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Pre-Mortemì„ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰í•´ë³¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ê·¸ëŠ” ì›ì¸ì´ ë˜ëŠ” ì§€ë‚˜ì¹œ ë‚™ê´€ì£¼ì˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Pre-Mortemì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ê°œë…ì„ ìš°ë¦¬ ë¬¸ì œì— ì ìš©í•´ë³´ë©´, ì•„ë˜ì™€ ê°™ì´ ì§„í–‰í•´ë³¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n1. íŒ€ì›ë“¤ê³¼ ëª¨ì—¬ì„œ 1,3,6ê°œì›” ë’¤ì— ì´ í…Œí¬ ìŠ¤í™ì´ ì°¸ë‹´í•˜ê²Œ ì‹¤íŒ¨í•˜ëŠ” ì´ìœ ë“¤ì„ ê°ì ì ì–´ë³¸ë‹¤.\n\n2. ê° ì‹¤íŒ¨ ì´ìœ ë“¤ì— ëŒ€í•´ ê³„íš ê³¼ì •ì—ì„œ ì˜ˆë°©í•  ë°©ë²•ì„ í™•ì¸í•œë‹¤.\n\n3. ê° ì‹¤íŒ¨ ì´ìœ ë“¤ì„ í•´ê²°í•œ ì™¸ë¶€ ì‚¬ë¡€ë¥¼ ê°„ë‹¨íˆ íƒìƒ‰í•œë‹¤.\n\nì´ë ‡ê²Œ ë˜ë©´, í•´ë‹¹ íŒ€ì´ ì§‘ë‹¨ì ìœ¼ë¡œ ê²°ì •ì— ìˆœì‘í•˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìœ¼ë©°, ë°•ì‹í•œ ì‚¬ëŒë“¤ì´ ë°”ëŒì§í•œ ë°©í–¥ìœ¼ë¡œ ìƒìƒë ¥ì„ í¼ì³ë‚´ ìœ„í—˜ ìš”ì†Œë¥¼ íŒŒì•…í•´ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\nì´ ê³¼ì •ì„ ê±°ì¹œ ë‹¤ìŒì—, ë‹¤ì‹œ ìœ„ì˜ 4ê°€ì§€ ê³ ë ¤ì‚¬í•­ì„ ìµœëŒ€í•œ ë§Œì¡±í•˜ëŠ” í…Œí¬ ìŠ¤í™ì„ ì„ ì •í•œë‹¤ë©´, í›¨ì”¬ ì‹ ë¢°ë„ ë†’ì€ í…Œí¬ ìŠ¤í™ì„ ì„ ì •í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n### 3. ë°ì´í„° Stageë¥¼ ê³„íší•©ë‹ˆë‹¤.\nê° Stageë³„ ì‹¤í–‰ë˜ëŠ” ETL ì‘ì—…ì˜ ëª©ì ì„ ì •ì˜í•˜ê³ , ê²°ê³¼ ë°ì´í„°ì˜ ì—­í• ì„ ì •ì˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´ data sourceë¥¼ ì ì¬í•˜ëŠ” data lakeì™€, ê°€ê³µëœ ë°ì´í„°ë“¤ì´ í¬í•¨ë˜ëŠ” data warehouse, ì‹¤ì œ product ë˜ëŠ” ë¶„ì„ì— ì‚¬ìš©ë  data martë¡œ stageë¥¼ êµ¬ë¶„í•´ ê³„íší•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ê° stageì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” pipeì˜ ì—­í• ì´ ëª…í™•í• ìˆ˜ë¡ ì¶”í›„ì— êµ¬ì¡° ê°œì„ ì„ í•  ë•Œì— ê°œì„  ì§€ì ì„ íŒŒì•…í•˜ê¸° ì¢‹ì„ ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\n\nê¸°ë³¸ì ìœ¼ë¡œ ì„¸ìš´ ì›ì¹™ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n1. ë°ì´í„° ì €ì¥ ê·œì¹™:\n\n    ê° stageëŠ” ë°ì´í„° ì €ì¥ ê·œì¹™ì„ ëª…í™•í•˜ê²Œ ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼í…Œë©´, ëª¨ë“  stageê°€ non-volatileí•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, SourceëŠ” Non-volatileì„ ê¼­ ì§€ì¼œì•¼ í•œë‹¤ëŠ” ê·œì¹™ì„ ì„¸ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n2. ë°ì´í„° ë¶„ë¥˜ ê·œì¹™:\n\n    ê° stageì˜ ë°ì´í„° ë¶„ë¥˜ ê·œì¹™ì„ ëª…í™•í•˜ê²Œ ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼í…Œë©´, data martì˜ ë°ì´í„° ì„±ê²©ì€ ì—¬ëŸ¬ ê°œì˜ data sourceë¥¼ í†µí•´ ìƒˆë¡œìš´ ì •ë³´ê°€ ìƒì„±ëœ ë°ì´í„°ì¼ ìˆ˜ë„ ìˆê³ , pipeline ì™¸ë¶€ì˜ ì‚¬ìš©ì²˜ê°€ ëª…í™•íˆ ì •í•´ì§„ ë°ì´í„°ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì„±ê²©ì„ ëª…í™•í•˜ê²Œ ë¶„ë¥˜í•˜ì§€ ì•Šì•„ data martì— í¬í•¨ë˜ì–´ì•¼ í•  ë°ì´í„°ê°€ ë‹¤ë¥¸ stageì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.\n\n3. ë°ì´í„° ê°€ê³µ ê·œì¹™:\n\n    ê° stageì˜ ë°ì´í„° ê°€ê³µ ê·œì¹™ì„ ëª…í™•í•˜ê²Œ ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼í…Œë©´, data mart ì´ì „ì˜ stageì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” pipelineì€ ë©±ë“±ì„±ì„ ë³´ì¥í•˜ëŠ” ë¡œì§ì„ ê°•ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜ëŠ”, data lakeì˜ ë°ì´í„°ë¥¼ ê°€ê³µí•  ë•Œì—ëŠ” ë°ì´í„° íƒ€ì… ë³€ê²½, ì¹¼ëŸ¼ ì´ë¦„ ë³€ê²½ ë“±ë§Œ í—ˆìš©í•˜ëŠ” ê·œì¹™ì„ ê°•ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n4. ì˜ˆì™¸ ì²˜ë¦¬ ê·œì¹™:\n\n    a,b,cì— ê´€í•œ ì˜ˆì™¸ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ íƒ€ë‹¹ì„±ì— ë§ê²Œ ëŒ€ì‘í•˜ë˜ ëª¨ë“  ì˜ˆì™¸ ì‚¬í•­ì€ re-architecturing pointë¡œ ê¸°ë¡í•´ revisit ê°€ëŠ¥í•˜ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤.\n\nì´ ê·œì¹™ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ê¹Œë‹¤ë¡œìš´ ì§€ì ì€ ë°ì´í„° ë¶„ë¥˜ ê·œì¹™ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ data martì˜ í…Œì´ë¸”ë“¤ì€ ê·¸ëŒ€ë¡œ Productì— ì‚¬ìš©ë˜ê¸°ë„ í•˜ì§€ë§Œ, mart í…Œì´ë¸” ì—¬ëŸ¬ ê°œë¥¼ joiní•´ì„œ ìƒˆë¡œìš´ í…Œì´ë¸”ë¡œ ë§Œë“¤ì–´ ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\nì´ëŸ¬í•œ ê²½ìš°ì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŒë‹¨ì„ í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n1. sourceë“¤ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë¶„ì„ ëª©ì  ìš©ë„ë¡œ ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ìƒì‚° ê°€ëŠ¥í•œ ì§€ë¥¼ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n2. ë¶„ì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš©í•  data mart í…Œì´ë¸”ì„ sourceë¡œ í•˜ëŠ” ë¶„ì„ ëª©ì ì˜ data warehouseë¥¼ ìƒˆë¡­ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” ì§€ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\nì´ ì™¸ì—ë„ ë¹„ì¦ˆë‹ˆìŠ¤ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” ì˜µì…˜ë“¤ì´ ìˆê² ìŠµë‹ˆë‹¤. ë§ì€ ê²½í—˜ê³¼ ê³µë¶€ë¥¼ í†µí•´ ë°ì´í„° ì—”ì§€ë‹ˆì–´ê°€ ì„±ì¥í•˜ëŠ” ì§€ì ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n### 4. ë°ì´í„° Lineage ê´€ë¦¬ ë°©ë²•ì„ ê³„íší•©ë‹ˆë‹¤.\në¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì´ ë³µì¡í•´ì§ˆìˆ˜ë¡, ìµœì´ˆì— ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ê³„íší•œ ì‘ì—…ìë“¤ìœ¼ë¡œ ì¸í•´ single point of failureê°€ ë°œìƒí•  í™•ë¥ ì´ ë†’ì•„ì§„ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëˆ„êµ¬ë‚˜ í•™ìŠµí•  ìˆ˜ ìˆëŠ” Lineage ê´€ë¦¬ ë°©ë²•ì„ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤. ëˆ„êµ¬ë‚˜ í•™ìŠµí•˜ê¸°ì—ëŠ” ì–´ë ¤ìš´ êµ¬ì¡°ë¼ê³  ìƒê°ì´ ë“¤ì—ˆë‹¤ë©´, ê²°êµ­ ì´ëŠ” SPFë¥¼ ë°œìƒì‹œí‚¤ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n\në°ì´í„° Lineageê°€ ìƒì‚°ì„±ì„ í•´ì¹˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ êµ¬ì„±ë˜ë ¤ë©´ ì•„ë˜ì˜ ìƒí™©ì€ ìµœëŒ€í•œ í”¼í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n1. í•œ data warehouse ë‚´ì—ì„œ cyclic êµ¬ì¡°ê°€ ìƒê²¨ì•¼ í•˜ëŠ” ê²½ìš°\n\n2. ì„œë¡œ ë‹¤ë¥¸ data stageì— í¬í•¨ëœ sourceë¥¼ ì°¸ì¡°í•´ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ëŠ” ê²½ìš°\n\nìœ„ ë‘ ê°€ì§€ ìƒí™©ì€ 3ë²ˆì—ì„œ ê³„íší•œ ë°ì´í„° stageì˜ ì •ì˜ë¥¼ í•´ì¹˜ê²Œ ë©ë‹ˆë‹¤.\nì´ëŸ¬í•œ ë¬¸ì œë“¤ì€ ë„ë©”ì¸ ì§€ì‹ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ, ì´í•´ë‹¹ì‚¬ìë“¤ê³¼ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ ì˜ í™œìš©í•´ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\në³µì¡ë„ê°€ ë†’ì€ ë°ì´í„° lineageëŠ” ê²°êµ­ ë‚˜ì¤‘ì— ë°ì´í„° ìƒì‚°ì„±ì˜ ë³‘ëª© ìš”ì¸ì´ ë  ê²ƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nì €ëŠ” ì—¬ê¸°ì—ì„œ ê¼­ í”¼í•´ì•¼ í•˜ëŠ” ì„ íƒë“¤ì´ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n1. ë°ì´í„° ë¶„ì„ê°€ ë˜ëŠ” ë°ì´í„° ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´ì˜ ì²˜ë¦¬ ë°©ì‹ì„ ë³€ê²½í•˜ëŠ” í•´ê²°ì±…:\n\n    ì´ í•´ê²°ì±…ì€ ê²°êµ­ ë‹¤ì‹œ ê°™ì€ ë¬¸ì œë¥¼ ì§ë©´í•˜ê²Œ í•  ê²ƒì…ë‹ˆë‹¤.\n\n2. ë¶ˆê°€í”¼í•˜ê²Œ ë³µì¡ë„ë¥¼ ì„ íƒí•œ ê²½ìš°ì—, í•´ë‹¹ ì„ íƒì˜ ê²°ê³¼ ë°ì´í„°ë¥¼ ì´í›„ ë°ì´í„°ì˜ sourceë¡œ í™œìš©í•˜ëŠ” ê²½ìš°:\n\n    ë³µì¡ë„ë¥¼ í”¼í•  ìˆ˜ ì—†ëŠ” êµ¬ì¡°ë¼ëŠ” ì´ìœ ë¡œ ì˜ˆì™¸ í—ˆìš©ì„ íƒí–ˆë‹¤ë©´, í—ˆìš©ëœ ì˜ˆì™¸ê°€ downstreamì—ì„œ ë³µì¡ë„ë¥¼ ë†’ì´ì§€ ëª»í•˜ê²Œ í†µì œí•´ì•¼ í•©ë‹ˆë‹¤.\n\n### 5. ë°ì´í„° Validation, Verification \u0026 Monitoring ë°©ë²•ì„ êµ¬ìƒí•©ë‹ˆë‹¤.\nì¢‹ì€ ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ì„¤ê³„í–ˆë‹¤ê³  í•˜ë”ë¼ë„, ëŠ˜ ìš°ë¦¬ëŠ” ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë°ì´í„°ë¥¼ ë§ˆì£¼í•´ì•¼ í•©ë‹ˆë‹¤.\nì´ëŸ¬í•œ ì˜ˆì™¸ëŠ” í• ë‹¹ëœ ë¦¬ì†ŒìŠ¤ ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ëŠ” ë³µì¡í•œ ë¡œì§ ë•Œë¬¸ì— ë°œìƒí•  ìˆ˜ë„ ìˆê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ ì£¼ì§€ ëª»í•˜ëŠ” outlierì— í•´ë‹¹í•˜ëŠ” Inputìœ¼ë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ë„ ìˆìœ¼ë©°, ë°ì´í„° sourceì˜ ìŠ¤í‚¤ë§ˆ ë³€ê²½ ë˜ëŠ” ë²„ì „ ì—…ê·¸ë ˆì´ë“œë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\në°ì´í„° ì—”ì§€ë‹ˆì–´ì—ê²Œ ì–´ë–¤ ê²ƒë³´ë‹¤ ì¤‘ìš”í•œ ê²ƒì€ ë°ì´í„° í€„ë¦¬í‹°ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„° ì—”ì§€ë‹ˆì–´ëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” íŒŒì´í”„ë¼ì¸ë“¤ì— ëŒ€í•´ì„œ ê°€ëŠ¥í•œ ì˜ˆì™¸ë¥¼ ê°€ì¥ ë¨¼ì € íŒŒì•…í•˜ê³  ëŒ€ì‘í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ì¢‹ì€ ë°ì´í„° ì•„í‚¤í…ì³ë¥¼ ì™„ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” Validationê³¼ Verificationì„ ì–´ë–»ê²Œ êµ¬ì„±í•  ì§€, ê·¸ë¦¬ê³  ì´ë¥¼ ì–´ë–»ê²Œ Monitoringí•  ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n\nì œê°€ Validation / Verificationì— ëŒ€í•´ì„œ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n1. Stageì˜ ì •ì˜ë¥¼ ë”°ë¥´ëŠ” ì§€ í™•ì¸í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n2. data lineageì— ë”°ë¼, Validationì„ í†µí•´ íŠ¹ì • pointì— ëŒ€í•´ ê° Validationì´ ë°ì´í„°ë¥¼ Verification í•  ìˆ˜ ìˆê²Œë” ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ë™ì¼í•œ ëª©ì ì˜ Validationì´ ì—¬ëŸ¬ stageì—ì„œ ìˆ˜í–‰ë  í•„ìš”ê°€ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n3. Validation ê³¼ì •ì—ì„œ íŒŒì•…ëœ ì˜ˆì™¸ ë°ì´í„°ë“¤ì€ ë³„ë„ë¡œ ê¸°ë¡ë˜ì–´, Verification Ruleì— í™œìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n4. Monitoringì€ ê°œë°œ ê³¼ì •ì—ì„œ ê°€ì¥ ì˜ˆì™¸ê°€ ë§ì´ ë°œìƒí•œ ì§€ì ì„ íŒŒì•…í•´ì„œ ì§„í–‰í•˜ë˜, ìš´ì˜ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” Issueë“¤ì„ í†µí•´ Grey Areaê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ë°œì „ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.\n\n## ê²°ë¡ \nì¢‹ì€ ë°ì´í„° ì•„í‚¤í…ì³ì— ëŒ€í•œ ì •ë‹µì€ ì—†ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ê³„ì†í•´ì„œ ë” ì¢‹ì€ ê¸°ìˆ ë“¤ì´ ì„¸ìƒì— ì†Œê°œë˜ê³  ìˆê³ , ë” ì°½ì˜ì ì¸ í•´ê²°ì±…ì´ ì œì‹œë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì €ëŠ” ê·¸ ì •ë‹µì„ ë§Œë“¤ì–´ë‚´ëŠ” ê³¼ì •ì—ì„œ ë†“ì¹˜ì§€ ì•Šì•„ì•¼ í•˜ëŠ” ê²ƒë“¤ì— ëŒ€í•´ì„œ ëŠ˜ ì£¼ì˜í•´ì•¼ í•œë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë‹¬ì„±í•œ ê²°ê³¼ë¬¼ì´ ìµœì„ ì´ ì•„ë‹ˆë¼ Second Bestì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ë§Œ Worstê°€ ì•„ë‹Œ Best Practiceë¥¼ ì§€í–¥í•˜ê³  ì´ë¥¼ ìœ„í•´ ê³ ë ¤í•  ì‚¬í•­ë“¤ì„ ìŠì§€ ì•Šì•„ì•¼ Another Bestë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/pandas_as_an_sql_en":{"title":"[DE] Pandas As an SQL (EN)","content":"\n\n## Pandas as an SQL\nDate : June 20, 2023\n\nI studied PANDAS as an SQL Statement\n```sql\nSELECT\n    table.a,\n    table.b as b1,\n    table2.c as c1,\n    table2.d,\n    sum(table2.e) as e1\nFROM table\nLEFT JOIN table2\nON table.a = table2.a\nWHERE true\n  AND table.a \u003e 0\n  AND (table.b = \"None\" or table2.c \u003e 0)\nGROUP BY table.a, table.b, table2.c, table2.d\nHAVING f1 \u003e 0\nORDER BY table.a ASC, table.b DESC, table2.c ASC\nLIMIT 100\nOFFSET 10\n```\nIf you were to write a query above in Pandas, you would write it like a code below:\n\n```python\nimport pandas as pd\n\ntable = pd.DataFrame(\n    {\n        'a': [1,2,3],\n        'b': [\"4\",\"4\",\"6\"],\n    }\n)\n\ntable2 = pd.DataFrame(\n        {\n                'a': [1,2,3],\n                'c': [1,2,3],\n                'd': [\"4\",\"6\",\"4\"],\n                'e': [1,2,3],\n        }\n)\n\nresult = (\n    table.merge(table2, on='a', how='left') # Join\n        [['a', 'b', 'c', 'd', 'e'] ] # Select\n        .rename(columns={'b': 'b1', 'c': 'c1'}) # Alias\n        .query('a \u003e 0 \u0026 (b1 == \"None\" | c1 \u003e 0)') # Where\n        .groupby(['a', 'b1', 'c1', 'd', 'e']) # Group by\n        .agg(\n                e1 = ('e', 'sum')\n        ) # Aggregation\n        .reset_index() # Reset Index - without this, the dataframe has multiple index consisted of columns which were used in group by statement\n        .query('e1 \u003e= 0') # Having\n        .sort_values(['a', 'b1', 'c1'], ascending=[True, False, True]) # Order by\n        .iloc[10:110] # limit and offset\n)\n\n```\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/data_engineering/pandas_as_an_sql_kr":{"title":"[DE] Pandas As an SQL (KR)","content":"\n\n## SQLì²˜ëŸ¼ ì“°ëŠ” Pandas\nì‘ì„± ì¼ì : 2023ë…„ 6ì›” 20ì¼\n\nPandasë¥¼ SQLë¬¸ì²˜ëŸ¼ ê³µë¶€í•´ë´¤ìŠµë‹ˆë‹¤.\n```sql\nSELECT\n    table.a,\n    table.b as b1,\n    table2.c as c1,\n    table2.d,\n    sum(table2.e) as e1\nFROM table\nLEFT JOIN table2\nON table.a = table2.a\nWHERE true\n  AND table.a \u003e 0\n  AND (table.b = \"None\" or table2.c \u003e 0)\nGROUP BY table.a, table.b, table2.c, table2.d\nHAVING f1 \u003e 0\nORDER BY table.a ASC, table.b DESC, table2.c ASC\nLIMIT 100\nOFFSET 10\n```\nì´ë¼ëŠ” ì¿¼ë¦¬ë¥¼ Pandasë¡œ ì‘ì„±í•œë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n\n```python\nimport pandas as pd\n\ntable = pd.DataFrame(\n    {\n        'a': [1,2,3],\n        'b': [\"4\",\"4\",\"6\"],\n    }\n)\n\ntable2 = pd.DataFrame(\n        {\n                'a': [1,2,3],\n                'c': [1,2,3],\n                'd': [\"4\",\"6\",\"4\"],\n                'e': [1,2,3],\n        }\n)\n\nresult = (\n    table.merge(table2, on='a', how='left') # Join\n        [['a', 'b', 'c', 'd', 'e'] ] # Select\n        .rename(columns={'b': 'b1', 'c': 'c1'}) # Alias\n        .query('a \u003e 0 \u0026 (b1 == \"None\" | c1 \u003e 0)') # Where\n        .groupby(['a', 'b1', 'c1', 'd', 'e']) # Group by\n        .agg(\n                e1 = ('e', 'sum')\n        ) # Aggregation\n        .reset_index() # Reset Index - without this, the dataframe has multiple index consisted of columns which were used in group by statement\n        .query('e1 \u003e= 0') # Having\n        .sort_values(['a', 'b1', 'c1'], ascending=[True, False, True]) # Order by\n        .iloc[10:110] # limit and offset\n)\n\n```\n","lastmodified":"2023-07-03T15:36:13.853775457Z","tags":null},"/notes/dune_nft/market_overview_en":{"title":"[Dune] NFT Market Overview Chart Review (EN)","content":"\n\n## Market Overview Chart Review\nDate : March 22, 2023\n\nReviewed [NFT Market Overview](https://dune.com/hildobby/NFTs)\n\n### 1. Overview\n\n![Screenshot](/notes/dune_nft/images/market_overview/01_overview.png)\n\nYou can see that we have constructed a pie chart using **Volume / Number of trades / Number of traders**.\n\nGiven that the amount of data is large by default, and that more recent data is more meaningful for trading, they show the most recent 1-week data.\nThe trade column in the **NFT Collection Ranked by Volume** chart below are links to actual tradeable websites, so they don't mean much.\n\n### 2. Volume\n\n![Screenshot](/notes/dune_nft/images/market_overview/02_volume.png)\n\nTrading volume is organized into Bar Chart and Stacked Area Chart, each of which allows you to view Daily Volume and Weekly Volume.\n\nThe Bar Chart is a great way to see total volume, so you can see overall volume trends and time series of marketplace share.\nThe Stacked Area Chart is an intuitive representation of the share seen in this Bar Chart.\n\n\n\n### 3. Transactions\n\n![Screenshot](/notes/dune_nft/images/market_overview/03_transactions.png)\n\nThe transactions chart is organized the same as the volume chart, with the numbers in each chart representing the number of transactions.\n\nIf you compare the number of trades chart to the volume chart, you can see that Blur's volume has recently spiked and surpassed Opensea's volume, but that hasn't translated to the number of trades. We can assume that there are still a lot of active users on Opensea.\n\n### 4. Traders\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_traders.png)\n\nThis chart shows the number of wallets that have traded NFTs.\n\nIn this case, They used a Line Chart instead of a Stacked Area Chart. I thought it's better to use a Stacked Area Chart, so I made Stacked Area Chart for comparison\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_1_traders_area.png)\n\nI was able to see these results, and there are a few takeaways from this comparison.\n\n1. Stacked Area Chart has the advantage of visualizing occupancy rates.\n2. Line charts allow you to visualize the share, but also to see how the value changes over time. For example, you can observe a sharp drop in the number of traders in March.\n3. However, the Traders chart is different from the other charts in that the Bar Chart counts the number of trading wallets by date, while the Line Chart counts the number of trading wallets by marketplace. Since there may be wallets that use multiple marketplaces at the same time, it is important to keep this in mind when organizing the chart.\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_2_traders_area_2.png)\nIf you compare the screenshot above with the Bar Chart in the Dashboard, you can see that the values on the y-axis are different.\n\n### 5. Token Standards\n\n![Screenshot](/notes/dune_nft/images/market_overview/05_token_standards.png)\n\nThese Charts show Volume, Trade Count, and Trader Count by Token Standard.\n\nThis is where the question arose.\nIn the case of ERC721, you can only trade one token per token id, whereas ERC1155 allows you to trade multiple.\nAlso, Opensea allows bundle trade, so I was wondering how to calculate the price of nfts in these situation.\n\nSo I looked into Dune Spellbook. [ë§í¬](https://dune.com/spellbook#!/model/model.spellbook.seaport_v2_ethereum_trades)\n![Screenshot](/notes/dune_nft/images/market_overview/06_spellbook_opensea.png)\n\nAs you can see in the screenshot above, if multiple NFTs were traded in a particular trade, we were able to categorize it as a bundle trade and then split the trade price equally.\nThis tells us the following:\n\n- The opensea trades included in the nft trades dataset used by Dune are calculated by dividing the total price of the individual purchases of nft in a trade by the number of nft in the trade.\n- As a result, some transaction value data was calculated incorrectly.\n- However, in bundle transactions, the seller estimates the individual prices of the NFTs in the bundle and sells them together, and the same applies to the buyer, so there is no problem with using this data unless there is an accounting requirement for both the seller and buyer.\n\n## Further Study\n\nWhile researching Spellbook, something occurred to me that I wanted to investigate further.\nI've seen that it's difficult to estimate the exact purchase price for bundle transactions on Opensea.\nI want to see how Dune Analytics estimate the purchase price for transactions from market aggregators like Gem and Blur that sweep listings from multiple marketplaces\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/notes/dune_nft/market_overview_kr":{"title":"[Dune] NFT Market Overview Chart Review (KR)","content":"\n\n## Market Overview ì°¨íŠ¸ ë¦¬ë·°\nì‘ì„± ì¼ì : 2023ë…„ 3ì›” 22ì¼\n\n[NFT Market Overview](https://dune.com/hildobby/NFTs)ë¥¼ ë¦¬ë·°í•´ë´¤ìŠµë‹ˆë‹¤.\n\n### 1. Overview\n\n![Screenshot](/notes/dune_nft/images/market_overview/01_overview.png)\n\n**ê±°ë˜ëŸ‰ / ê±°ë˜ íšŸìˆ˜ / ê±°ë˜ì ìˆ˜**ë¥¼ ì´ìš©í•´ íŒŒì´ì°¨íŠ¸ë¥¼ êµ¬ì„±í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\në°ì´í„° ì–‘ì´ ê¸°ë³¸ì ìœ¼ë¡œ ë§ê³ , ìµœì‹  ë°ì´í„°ê°€ Tradingì— ì¢€ ë” ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ì ì„ ê°ì•ˆí•´, ìµœê·¼ 1ì£¼ ë°ì´í„°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\ní•˜ë‹¨ **NFT Collection Ranked by Volume** ì°¨íŠ¸ì˜ TradeëŠ” ì‹¤ì œ trade ê°€ëŠ¥í•œ ì›¹ì‚¬ì´íŠ¸ì˜ ë§í¬ë¼ì„œ í° ì˜ë¯¸ëŠ” ì—†ìŠµë‹ˆë‹¤.\n\n### 2. Volume\n\n![Screenshot](/notes/dune_nft/images/market_overview/02_volume.png)\n\nê±°ë˜ëŸ‰ì€ Bar Chartì™€ Stacked Area Chartë¡œ êµ¬ì„±ë˜ì–´ ìˆê³ , ê°ê°ì€ Daily Volumeê³¼ Weekly Volumeì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nBar ChartëŠ” ê±°ë˜ëŸ‰ ì´í•©ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì „ë°˜ì ì¸ ê±°ë˜ëŸ‰ ì¶”ì´ì™€ ì‹œê³„ì—´ë¡œ Marketplace ë³„ ì ìœ ìœ¨ì„ ê°€ëŠ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nStacked Area ChartëŠ” ì´ Bar Chartì—ì„œ í™•ì¸í•œ ì ìœ ìœ¨ì„ ì§ê´€ì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤.\n\n### 3. Transactions\n\n![Screenshot](/notes/dune_nft/images/market_overview/03_transactions.png)\n\nê±°ë˜ íšŸìˆ˜ëŠ” ê±°ë˜ëŸ‰ ì°¨íŠ¸ì™€ ë™ì¼í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆê³ , ê° ì°¨íŠ¸ì˜ ìˆ˜ì¹˜ëŠ” ê±°ë˜ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\nê±°ë˜ íšŸìˆ˜ ì°¨íŠ¸ë¥¼ ê±°ë˜ëŸ‰ ì°¨íŠ¸ì™€ ë¹„êµí•´ì„œ ë³´ë©´, ìµœê·¼ Blurì˜ ê±°ë˜ëŸ‰ì´ ê¸‰ë“±í•´ Openseaì˜ ê±°ë˜ëŸ‰ì„ í¬ê²Œ ë„˜ì–´ì„°ì§€ë§Œ, ê·¸ê²ƒì´ ê±°ë˜ íšŸìˆ˜ê¹Œì§€ ì´ì–´ì§€ì§€ëŠ” ì•Šì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì§ Openseaì˜ í™œì„±í™” ìœ ì €ê°€ ë§ì´ ë‚¨ì•„ìˆìŒì„ ì¶”ì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### 4. Traders\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_traders.png)\n\nì´ ChartëŠ” NFTë¥¼ ê±°ë˜í•œ ì§€ê°‘ì˜ ìˆ˜ ì¶”ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\nì´ ê²½ìš°ì—ëŠ” Stacked Area Chart ëŒ€ì‹ , Line Chartë¥¼ ì‚¬ìš©í–ˆëŠ”ë°, Stacked Area Chartë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´ ë¹„êµí•´ ë³´ê³ ì ì§ì ‘ Chartë¥¼ ì‘ì„±í•´ ë´¤ìŠµë‹ˆë‹¤.\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_1_traders_area.png)\n\nì´ëŸ¬í•œ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ ì°¨íŠ¸ì™€ ê¸°ì¡´ ì°¨íŠ¸ë¥¼ ë¹„êµí•´ ë³´ê³  ëª‡ ê°€ì§€ takeawayë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n1. Stacked Area Chartë¥¼ ì‚¬ìš©í•˜ë©´ ì ìœ ìœ¨ì„ ì‹œê°í™” í•˜ëŠ” ê²ƒì— ì¥ì ì´ ìˆë‹¤.\n2. Line Chartë¥¼ ì‚¬ìš©í•˜ë©´ ì ìœ ìœ¨ì„ ì‹œê°í™”í•˜ëŠ” ë™ì‹œì—, ì‹œê³„ì—´ì— ë”°ë¥¸ ê°’ ë³€í™” ì°¨ì´ë¥¼ ì˜ ë³¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 3ì›”ì— ê¸‰ê²©íˆ ê±°ë˜ì ìˆ˜ê°€ ê¸‰ê°í•œ ì‹œê¸°ë¥¼ ê´€ì°°í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\n3. ë‹¤ë§Œ Traders ì°¨íŠ¸ëŠ” ë‹¤ë¥¸ ì°¨íŠ¸ì™€ ë‹¬ë¦¬ ì£¼ì˜í•  ì ì´ ìˆì—ˆëŠ”ë°, Bar Chartì˜ ê²½ìš°ëŠ” ë‚ ì§œë³„ ê±°ë˜ ì§€ê°‘ ìˆ˜ë¥¼ ì„¼ ë°˜ë©´, Line Chartì—ì„œëŠ” Marketplace ë³„ë¡œ ê±°ë˜ ì§€ê°‘ ìˆ˜ë¥¼ ì„¸ê³  ìˆë‹¤. ì—¬ëŸ¬ Marketplaceë¥¼ ë™ì‹œì— ì´ìš©í•œ ì§€ê°‘ì´ ì¡´ì¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì°¨íŠ¸ êµ¬ì„± ì‹œ ìœ ì˜í•˜ë©´ ì¢‹ë‹¤.\n\n![Screenshot](/notes/dune_nft/images/market_overview/04_2_traders_area_2.png)\nìœ„ ìŠ¤í¬ë¦°ìƒ·ê³¼ Dashboardì˜ Bar Chartë¥¼ ë¹„êµí•´ë³´ë©´ yì¶•ì˜ ê°’ì´ ë‹¤ë¥¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### 5. Token Standards\n\n![Screenshot](/notes/dune_nft/images/market_overview/05_token_standards.png)\n\nToken Standard ë³„ë¡œ Volume, Trade Count, Trader Countë¥¼ í‘œê¸°í•œ ê²ƒì…ë‹ˆë‹¤.\n\nì—¬ê¸°ì—ì„œ ê¶ê¸ˆì¦ì´ ìƒê²¼ìŠµë‹ˆë‹¤.\nEERC721ì˜ ê²½ìš°ì—ëŠ” token id ë³„ë¡œ 1ê°œë§Œ ê±°ë˜ ê°€ëŠ¥í•œ ë°ì— ë°˜í•´, ERC1155ëŠ” ì—¬ëŸ¬ ê°œë¥¼ ê±°ë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.RC721ì˜ ê²½ìš°ì—ëŠ” 1ê°œë§Œ ê±°ë˜í•˜ëŠ” ë°ì— ë°˜í•´, ERC1155ëŠ” ì—¬ëŸ¬ê°œë¥¼ ê±°ë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në”ë¶ˆì–´, Openseaì˜ ê²½ìš°, Bundle ê±°ë˜ê°€ ê°€ëŠ¥í•´ ê·¸ ê±°ë˜ Volumeì„ ì–´ë–»ê²Œ ì¡ëŠ” ì§€ ê¶ê¸ˆí–ˆìŠµë‹ˆë‹¤.\n\nê·¸ë˜ì„œ Spellbookì„ ì¡°ì‚¬í•´ë´¤ìŠµë‹ˆë‹¤. [ë§í¬](https://dune.com/spellbook#!/model/model.spellbook.seaport_v2_ethereum_trades)\n![Screenshot](/notes/dune_nft/images/market_overview/06_spellbook_opensea.png)\n\nìœ„ ìŠ¤í¬ë¦°ìƒ·ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, íŠ¹ì • ê±°ë˜ì—ì„œ nftê°€ ì—¬ëŸ¬ ê°œ ê±°ë˜ë˜ëŠ” ê²½ìš°ì—ëŠ” bundle ê±°ë˜ë¡œ ë¶„ë¥˜í•œ ë‹¤ìŒ, ê±°ë˜ ëŒ€ê¸ˆì„ ê· ë“±í•˜ê²Œ ë‚˜ëˆ„ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\nì´ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- Duneì—ì„œ ì‚¬ìš©í•˜ëŠ” nft trades datasetì— í¬í•¨ëœ opensea ê±°ë˜ëŠ” í•œ ê±°ë˜ì— í¬í•¨ëœ nftë“¤ì˜ ê°œë³„ êµ¬ë§¤ ëŒ€ê¸ˆì„ ì „ì²´ ëŒ€ê¸ˆì—ì„œ ê±°ë˜ì— í¬í•¨ëœ nftì˜ ê°œìˆ˜ë¡œ ë‚˜ëˆ ì„œ ê³„ì‚°í•œë‹¤.\n- ë”°ë¼ì„œ, ì¼ë¶€ ê±°ë˜ ëŒ€ê¸ˆ ë°ì´í„°ëŠ” ë¶€ì •í™•í•˜ê²Œ ê³„ì‚°ë˜ì—ˆë‹¤.\n- ë‹¤ë§Œ, ì‹¤ì œë¡œë„ Bundle ê±°ë˜ê°€ ì¼ì–´ë‚  ê²½ìš°ì—ëŠ” íŒë§¤ì ì—­ì‹œ Bundleì— í¬í•¨ëœ NFTë“¤ì˜ ê°œë³„ ê°€ê²©ì„ ì¶”ì •í•œ ë‹¤ìŒ í•©ì‚°í•´ì„œ íŒë§¤í•˜ê³ , ì´ëŠ” êµ¬ë§¤ì ì…ì¥ì—ì„œë„ ë™ì¼í•˜ê²Œ ì ìš©ë˜ê¸° ë•Œë¬¸ì—, ì‹¤ì œ íŒë§¤ìì™€ êµ¬ë§¤ì ì…ì¥ì—ì„œëŠ” íšŒê³„ ì²˜ë¦¬ê°€ í•„ìš”í•œ ìƒí™©ì´ ì•„ë‹ˆë¼ë©´ ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì— í° ë¬¸ì œê°€ ì—†ë‹¤.\n\n## Further Study\n\nSpellbookì„ ì¡°ì‚¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì¢€ ë” ì¡°ì‚¬í•˜ê³  ì‹¶ì€ ê²ƒì´ ìƒê²¼ìŠµë‹ˆë‹¤.\nOpenseaì—ì„œ ë°œìƒí•œ Bundle ê±°ë˜ëŠ” ì •í™•í•œ êµ¬ë§¤ ëŒ€ê¸ˆì„ ì¶”ì •í•˜ê¸° ì–´ë µë‹¤ëŠ” ì‚¬ì‹¤ì€ í™•ì¸í–ˆëŠ”ë°,\nGem, Blurì™€ ê°™ì€ Market Aggregatorì—ì„œ ì—¬ëŸ¬ Marketplaceì˜ Listingì„ Sweepí•˜ëŠ” ê±°ë˜ì˜ ê²½ìš°ëŠ” ì–´ë–»ê²Œ êµ¬ë§¤ ëŒ€ê¸ˆì„ ì‚°ì •í•˜ëŠ” ì§€ í™•ì¸í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/private/study/algorithm":{"title":"Algorithm","content":"\n\n## ì´ì§„íƒìƒ‰ ë¬¸ì œ\n- êµ¬í•´ì•¼ í•˜ëŠ” ê°’ê³¼, ê·¸ ê°’ì— ë”°ë¼ ì •í•´ì§€ëŠ” ë³€ìˆ˜ ê°’ë“¤ì— ëŒ€í•œ ìµœì í™”ê°€ í•„ìš”í•  ë•Œ ë°©ì •ì‹ ë˜ëŠ” ë¶€ë“±ì‹ì˜ í˜•íƒœë¡œ ì •ì˜í•  ìˆ˜ ìˆëŠ” ë¬¸ì œëŠ” ì´ì§„ íƒìƒ‰ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤.\n- ê°€ì¥ ëŒ€í‘œì ì¸ ë¬¸ì œëŠ” ëŒ€ê¸°ì—´ì— ì„œìˆëŠ” nëª…ì˜ ì‚¬ëŒë“¤ê³¼ ì´ ëŒ€ê¸°ì—´ì„ ì†Œí™”í•  ìˆ˜ ìˆëŠ” ê°ê¸° ë‹¤ë¥¸ ì²˜ë¦¬ ì‹œê°„ì„ ê°€ì§„ ì²˜ë¦¬ê¸° ì§‘í•©ì„ ë†“ê³ , ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœì†Œ ì‹œê°„ì„ êµ¬í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\n- ì´ ê²½ìš°ì—ëŠ” ìµœì†Œ ì‹œê°„ì„ ëª©ì ê°’ìœ¼ë¡œ í•˜ê³ , ì´ ìµœì†Œ ì‹œê°„ì— ë”°ë¼ ê° ì²˜ë¦¬ê¸°ê°€ ì²˜ë¦¬í•˜ëŠ” ì‚¬ëŒì˜ ìˆ˜ë¥¼ ì—­ìœ¼ë¡œ ì°¾ëŠ” ê³¼ì •ì—ì„œ íš¨ìœ¨ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ìµœì†Œ ì‹œê°„ì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê°’ì—ì„œë¶€í„° ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì†Œ ì‹œê°„ì„ ì¶”ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ì„ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/private/study/data_engineering":{"title":"Airflow","content":"\n\n## Airflow on Kubernetes ë°°í¬í•˜ê¸° (on M1 Mac)\n- m1ì— minikube ì„¤ì¹˜í•˜ê¸° [ì°¸ì¡°](https://velog.io/@pinion7/macOs-m1-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-kubernetes-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0)\n    ```shell\n    $ curl -Lo minikube https://github.com/kubernetes/minikube/releases/download/v1.25.1/minikube-darwin-arm64 \\\n  \u0026\u0026 chmod +x minikube\n    ```\n    ```shell\n    $ sudo install minikube /usr/local/bin/minikube\n    ```\n- heml, kubectl ì„¤ì¹˜í•˜ê¸°\n    ```shell\n    $ brew install helm, kubectl\n    ```\n- minikube ì‹œì‘\n    ```shell\n    $ minikube start --driver=docker\n    ```\n- helm repo ì¶”ê°€\n    ```shell\n    $ helm repo add apache-airflow https://airflow.apache.org\n    $ helm repo update\n    ```\n- airflow chart ì„¤ì¹˜\n    ```shell\n    $ helm install $RELEASE_NAME apache-airflow/airflow --namespace $NAMESPACE --debug\n    ```\n- webserver port-forwarding\n    ```shell\n    $ kubectl port-forward svc/airflow-webserver 8080:8080 --namespace $NAMESPACE\n    ```\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null},"/private/study/sql":{"title":"SQL","content":"\n\n## Rollup Query\n- ì‚¬ìš© ë°©ë²•: SUM(COLUMN_2) ... GROUP BY COLUMN_1, ROLLUP(COLUMN_1)\n- ê²°ê³¼: Column 1 ë³„ Column 2ì˜ SUM ê²°ê³¼ë“¤ì´ í‘œí˜„ë˜ë©´ì„œ ê·¸ ë‹¤ìŒ Rowì— SUM ê²°ê³¼ë“¤ì˜ í•©ë„ í‘œí˜„ëœë‹¤.\n","lastmodified":"2023-07-03T15:36:13.865775569Z","tags":null}}